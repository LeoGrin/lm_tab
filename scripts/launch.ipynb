{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loading import load_data\n",
    "from skrub import MinHashEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from src.utils import FeaturesExtractor, FixedSizeSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(X, encoder_name):\n",
    "    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "        X = np.array(X)\n",
    "    encoder_type, encoder_params = encoder_name.split(\"__\", 1)\n",
    "    if encoder_type == \"lm\":\n",
    "        encoder = SentenceTransformer(encoder_params)\n",
    "        return encoder.encode(X)\n",
    "    elif encoder_type == \"skrub\":\n",
    "        if encoder_params.startswith(\"minhash\"):\n",
    "            n_components = int(encoder_params.split(\"_\")[1])\n",
    "            encoder = MinHashEncoder(n_components=n_components)\n",
    "            # reshape to 2d array\n",
    "            # if pandas dataframe, convert to numpy array\n",
    "            X = X.reshape(-1, 1)\n",
    "            return encoder.fit_transform(X)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown skrub encoder {encoder_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Number of iterations:  756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: journal_jcr_cls, Encoding: skrub__minhash_10\n",
      "Original task: classification for journal_jcr_cls\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0029447078704833984\n",
      "Dataset: journal_jcr_cls, Encoding: skrub__minhash_30\n",
      "Original task: classification for journal_jcr_cls\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0011892318725585938\n",
      "Dataset: journal_jcr_cls, Encoding: lm__all-MiniLM-L12-v2\n",
      "Original task: classification for journal_jcr_cls\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.008070945739746094\n",
      "Dataset: journal_jcr_cls, Encoding: lm__whaleloops/phrase-bert\n",
      "Original task: classification for journal_jcr_cls\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:34<04:39, 34.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.007950305938720703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: movies, Encoding: skrub__minhash_10\n",
      "Original task: regression for movies\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0012049674987792969\n",
      "Dataset: movies, Encoding: skrub__minhash_30\n",
      "Original task: regression for movies\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0011186599731445312\n",
      "Dataset: movies, Encoding: lm__all-MiniLM-L12-v2\n",
      "Original task: regression for movies\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Time elapsed: 0.010854244232177734\n",
      "Dataset: movies, Encoding: lm__whaleloops/phrase-bert\n",
      "Original task: regression for movies\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [01:07<03:55, 33.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Time elapsed: 0.008118867874145508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: michelin, Encoding: skrub__minhash_10\n",
      "Original task: classification for michelin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.001201629638671875\n",
      "Dataset: michelin, Encoding: skrub__minhash_30\n",
      "Original task: classification for michelin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0011665821075439453\n",
      "Dataset: michelin, Encoding: lm__all-MiniLM-L12-v2\n",
      "Original task: classification for michelin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.00830531120300293\n",
      "Dataset: michelin, Encoding: lm__whaleloops/phrase-bert\n",
      "Original task: classification for michelin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [01:40<03:18, 33.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.008167505264282227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: spotify, Encoding: skrub__minhash_10\n",
      "Original task: classification for spotify\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0011970996856689453\n",
      "Dataset: spotify, Encoding: skrub__minhash_30\n",
      "Original task: classification for spotify\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0012073516845703125\n",
      "Dataset: spotify, Encoding: lm__all-MiniLM-L12-v2\n",
      "Original task: classification for spotify\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Time elapsed: 0.011551141738891602\n",
      "Dataset: spotify, Encoding: lm__whaleloops/phrase-bert\n",
      "Original task: classification for spotify\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [02:14<02:48, 33.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.008079290390014648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: employee_salary, Encoding: skrub__minhash_10\n",
      "Original task: regression for employee_salary\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0013606548309326172\n",
      "Dataset: employee_salary, Encoding: skrub__minhash_30\n",
      "Original task: regression for employee_salary\n",
      "Converting to binary classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0010266304016113281\n",
      "Dataset: employee_salary, Encoding: lm__all-MiniLM-L12-v2\n",
      "Original task: regression for employee_salary\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.011063814163208008\n",
      "Dataset: employee_salary, Encoding: lm__whaleloops/phrase-bert\n",
      "Original task: regression for employee_salary\n",
      "Converting to binary classification\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      " 56%|█████▌    | 5/9 [02:36<01:57, 29.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.008634090423583984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: drug_directory, Encoding: skrub__minhash_10\n",
      "Original task: classification for drug_directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0008366107940673828\n",
      "Dataset: drug_directory, Encoding: skrub__minhash_30\n",
      "Original task: classification for drug_directory\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0012755393981933594\n",
      "Dataset: drug_directory, Encoding: lm__all-MiniLM-L12-v2\n",
      "Original task: classification for drug_directory\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.008371114730834961\n",
      "Dataset: drug_directory, Encoding: lm__whaleloops/phrase-bert\n",
      "Original task: classification for drug_directory\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [03:40<02:03, 41.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.008210182189941406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: museums, Encoding: skrub__minhash_10\n",
      "Original task: regression for museums\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0014157295227050781\n",
      "Dataset: museums, Encoding: skrub__minhash_30\n",
      "Original task: regression for museums\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0010960102081298828\n",
      "Dataset: museums, Encoding: lm__all-MiniLM-L12-v2\n",
      "Original task: regression for museums\n",
      "Converting to binary classification\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.007586240768432617\n",
      "Dataset: museums, Encoding: lm__whaleloops/phrase-bert\n",
      "Original task: regression for museums\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [04:13<01:17, 38.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.008290529251098633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: fifa_footballplayers_22, Encoding: skrub__minhash_10\n",
      "Original task: regression for fifa_footballplayers_22\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0011794567108154297\n",
      "Dataset: fifa_footballplayers_22, Encoding: skrub__minhash_30\n",
      "Original task: regression for fifa_footballplayers_22\n",
      "Converting to binary classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0011909008026123047\n",
      "Dataset: fifa_footballplayers_22, Encoding: lm__all-MiniLM-L12-v2\n",
      "Original task: regression for fifa_footballplayers_22\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.008221149444580078\n",
      "Dataset: fifa_footballplayers_22, Encoding: lm__whaleloops/phrase-bert\n",
      "Original task: regression for fifa_footballplayers_22\n",
      "Converting to binary classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      " 89%|████████▉ | 8/9 [04:44<00:36, 36.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.008066654205322266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: jp_anime, Encoding: skrub__minhash_10\n",
      "Original task: regression for jp_anime\n",
      "Converting to binary classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0008862018585205078\n",
      "Dataset: jp_anime, Encoding: skrub__minhash_30\n",
      "Original task: regression for jp_anime\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.001291513442993164\n",
      "Dataset: jp_anime, Encoding: lm__all-MiniLM-L12-v2\n",
      "Original task: regression for jp_anime\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.00855255126953125\n",
      "Dataset: jp_anime, Encoding: lm__whaleloops/phrase-bert\n",
      "Original task: regression for jp_anime\n",
      "Converting to binary classification\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [05:14<00:00, 34.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.009312152862548828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encodings = [\"skrub__minhash_10\", \"skrub__minhash_30\", \"lm__all-MiniLM-L12-v2\", \"lm__whaleloops/phrase-bert\"]\n",
    "datasets = [\"journal_jcr_cls\", \"movies\", \"michelin\", \"spotify\", \"employee_salary\", \"drug_directory\", \"museums\", \"fifa_footballplayers_22\", \"jp_anime\"]\n",
    "dim_reductions = {\"PCA_10\": PCA(n_components=10), \"PCA_30\": PCA(n_components=10), \"subset_10\": FeaturesExtractor(method=\"first\", n_features=10), \"subset_30\": FeaturesExtractor(method=\"first\", n_features=30),\n",
    "                  \"subset_biggest_10\": FeaturesExtractor(method=\"biggest_variance\", n_features=10), \"subset_biggest_30\": FeaturesExtractor(method=\"biggest_variance\", n_features=30),\n",
    "                  \"passthrough\": \"passthrough\"}\n",
    "models = {\"LogisticRegression\": LogisticRegression(), \"GradientBoostingClassifier\": GradientBoostingClassifier(), \"TabPFNClassifier\": TabPFNClassifier(device=\"cpu\")}\n",
    "print(\"Number of iterations: \", len(datasets) * len(encodings) * len(dim_reductions) * len(models))\n",
    "\n",
    "def run_on_encoded_data(X_enc, y, dim_reduction_name, dim_reduction, model_name, model):\n",
    "    if dim_reduction_name == \"passthrough\" and model_name != \"LogisticRegression\" and not encoding.startswith(\"skrub\"):\n",
    "        return None\n",
    "    if dim_reduction_name != \"passthrough\" and encoding.startswith(\"skrub\"):\n",
    "        return None\n",
    "    pipeline = Pipeline([(\"dim_reduction\", dim_reduction), (\"model\", model)])\n",
    "    cv = FixedSizeSplit(n_splits=5, n_train=1000, n_test=4000)\n",
    "    scores = cross_val_score(pipeline, X_enc, y, scoring=\"accuracy\", cv=cv)\n",
    "    return scores\n",
    "\n",
    "results = pd.DataFrame(columns=[\"dataset\", \"encoding\", \"dim_reduction\", \"model\", \"accuracy\"])\n",
    "\n",
    "for dataset in tqdm(datasets):\n",
    "    for encoding in tqdm(encodings, leave=False):\n",
    "        #TODO do a proper sklearn transform (not a problem for sentence transformers)\n",
    "        # right now not done for speed reasons and gpu handling\n",
    "        print(f\"Dataset: {dataset}, Encoding: {encoding}\")\n",
    "        X, y = load_data(dataset, max_rows=10000)\n",
    "        X_enc = encode(X, encoding)\n",
    "\n",
    "        # # run with joblib\n",
    "        results_data_enc = Parallel(n_jobs=-1)(delayed(run_on_encoded_data)(X_enc, y, dim_reduction_name, dim_reduction,model_name, model) for (dim_reduction_name, dim_reduction) in dim_reductions.items() for (model_name, model) in models.items())\n",
    "        start_time = time.time()\n",
    "        for dim_reduction_name, dim_reduction in dim_reductions.items():\n",
    "            for model_name, model in models.items():\n",
    "                scores = results_data_enc.pop(0)\n",
    "                if scores is None:\n",
    "                    continue\n",
    "                results = pd.concat([results, pd.DataFrame({\"dataset\": dataset, \"encoding\": encoding, \"dim_reduction\": dim_reduction_name, \"model\": model_name, \"accuracy\": scores})])\n",
    "        print(f\"Time elapsed: {time.time() - start_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "#results.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>encoding</th>\n",
       "      <th>dim_reduction</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>journal_jcr_cls</td>\n",
       "      <td>skrub__minhash_10</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.53550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>journal_jcr_cls</td>\n",
       "      <td>skrub__minhash_10</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.54750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>journal_jcr_cls</td>\n",
       "      <td>skrub__minhash_10</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.54550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>journal_jcr_cls</td>\n",
       "      <td>skrub__minhash_10</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.54525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>journal_jcr_cls</td>\n",
       "      <td>skrub__minhash_10</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.53850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>jp_anime</td>\n",
       "      <td>lm__whaleloops/phrase-bert</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.54050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>jp_anime</td>\n",
       "      <td>lm__whaleloops/phrase-bert</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.55975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>jp_anime</td>\n",
       "      <td>lm__whaleloops/phrase-bert</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.56300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>jp_anime</td>\n",
       "      <td>lm__whaleloops/phrase-bert</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.56100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>jp_anime</td>\n",
       "      <td>lm__whaleloops/phrase-bert</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.55825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dataset                    encoding dim_reduction  \\\n",
       "0     journal_jcr_cls           skrub__minhash_10   passthrough   \n",
       "1     journal_jcr_cls           skrub__minhash_10   passthrough   \n",
       "2     journal_jcr_cls           skrub__minhash_10   passthrough   \n",
       "3     journal_jcr_cls           skrub__minhash_10   passthrough   \n",
       "4     journal_jcr_cls           skrub__minhash_10   passthrough   \n",
       "...               ...                         ...           ...   \n",
       "1795         jp_anime  lm__whaleloops/phrase-bert   passthrough   \n",
       "1796         jp_anime  lm__whaleloops/phrase-bert   passthrough   \n",
       "1797         jp_anime  lm__whaleloops/phrase-bert   passthrough   \n",
       "1798         jp_anime  lm__whaleloops/phrase-bert   passthrough   \n",
       "1799         jp_anime  lm__whaleloops/phrase-bert   passthrough   \n",
       "\n",
       "                   model  accuracy  \n",
       "0     LogisticRegression   0.53550  \n",
       "1     LogisticRegression   0.54750  \n",
       "2     LogisticRegression   0.54550  \n",
       "3     LogisticRegression   0.54525  \n",
       "4     LogisticRegression   0.53850  \n",
       "...                  ...       ...  \n",
       "1795  LogisticRegression   0.54050  \n",
       "1796  LogisticRegression   0.55975  \n",
       "1797  LogisticRegression   0.56300  \n",
       "1798  LogisticRegression   0.56100  \n",
       "1799  LogisticRegression   0.55825  \n",
       "\n",
       "[1800 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "results = pd.read_csv(\"../results/results.csv\")\n",
    "#esults['mean_accuracy'] = results['accuracy'].apply(np.mean)\n",
    "#melted_results = results\n",
    "# \"Melting\" the data to make it suitable for a swarmplot\n",
    "#melted_results = results.explode('accuracy')\n",
    "#melted_results['accuracy'] = melted_results['accuracy'].astype(float)\n",
    "# average the accuracy scores in the accuracy column\n",
    "\n",
    "# filter for encoding =lm__all-MiniLM-L12-v2\n",
    "# take the mean of accuracy\n",
    "melted_results = results.groupby(['dataset', 'model', 'dim_reduction', 'encoding']).mean().reset_index()\n",
    "#melted_results = results.explode('accuracy')\n",
    "#melted_results['accuracy'] = melted_results['accuracy'].astype(float)\n",
    "melted_results = melted_results[melted_results['encoding'] == 'lm__all-MiniLM-L12-v2']\n",
    "print(len(melted_results))\n",
    "\n",
    "# Creating the swarmplot\n",
    "# plt.figure(figsize=(15, 20))\n",
    "# sns.swarmplot(data=melted_results, x='accuracy', y='dataset', hue='model', dodge=True)\n",
    "# plt.title('Swarm Plot of Model Accuracies Across Datasets')\n",
    "# plt.xlabel('Accuracy')\n",
    "# plt.ylabel('Dataset')\n",
    "# plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# plt.show()\n",
    "# Create the plot\n",
    "fig = px.strip(\n",
    "    data_frame=melted_results,\n",
    "    x=\"accuracy\",\n",
    "    y=\"dataset\",\n",
    "    color=\"model\",\n",
    "    title=\"Swarm Plot of Model Accuracies Across Datasets\",\n",
    "    labels={\"accuracy\": \"Accuracy\", \"dataset\": \"Dataset\"},\n",
    "    height=600,\n",
    "    width=900,\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original task: classification for spotify\n"
     ]
    }
   ],
   "source": [
    "# compare speed on cpu and gpu\n",
    "X, y = load_data(\"spotify\", max_rows=10000)\n",
    "# label encoding\n",
    "#y = y.astype('category').cat.codes\n",
    "#y = y.astype(np.int64)\n",
    "#X_enc = encode(X, \"lm__all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a tokenizer and BERT model that work together\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import LRScheduler, ProgressBar\n",
    "from skorch.hf import HuggingfacePretrainedTokenizer\n",
    "# import LambdaLR from torch.optim.lr_scheduler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from src.models import BertAndTabPFN\n",
    "\n",
    "TOKENIZER = \"distilbert-base-uncased\"\n",
    "PRETRAINED_MODEL = \"distilbert-base-uncased\"\n",
    "\n",
    "# model hyper-parameters\n",
    "OPTMIZER = torch.optim.AdamW\n",
    "LR = 5e-5\n",
    "MAX_EPOCHS = 3\n",
    "CRITERION = nn.CrossEntropyLoss\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "num_training_steps = MAX_EPOCHS * (len(X) // BATCH_SIZE + 1)\n",
    "\n",
    "def lr_schedule(current_step):\n",
    "    factor = float(num_training_steps - current_step) / float(max(1, num_training_steps))\n",
    "    assert factor > 0\n",
    "    return factor\n",
    "\n",
    "class BertModule(nn.Module):\n",
    "    def __init__(self, name, num_labels):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.reset_weights()\n",
    "        \n",
    "    def reset_weights(self):\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.name, num_labels=self.num_labels\n",
    "        )\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        pred = self.bert(**kwargs)\n",
    "        return pred.logits\n",
    "    \n",
    "class BertAndTabPFNWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, X, **fit_params):\n",
    "        y = fit_params.get(\"y\", None)\n",
    "        return self.model(X, y)\n",
    "    \n",
    "#wrapped_model = BertAndTabPFNWrapper(BertAndTabPFN)\n",
    "\n",
    "class HuggingfacePretrainedTokenizerWithY(HuggingfacePretrainedTokenizer):\n",
    "    def transform(self, X):\n",
    "        res = super().transform(X)\n",
    "        res[\"y\"] = y\n",
    "        return res\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tokenizer', HuggingfacePretrainedTokenizerWithY(TOKENIZER)),\n",
    "    ('net', NeuralNetClassifier(\n",
    "        BertAndTabPFN,\n",
    "        module__dim_tabpfn=50, \n",
    "        module__preprocess_before_tabpfn=False,\n",
    "        #module__name=PRETRAINED_MODEL,\n",
    "        #module__num_labels=len(set(y)),\n",
    "        optimizer=OPTMIZER,\n",
    "        lr=LR,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        criterion=CRITERION,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        iterator_train__shuffle=True,\n",
    "        device=DEVICE,\n",
    "        callbacks=[\n",
    "            LRScheduler(LambdaLR, lr_lambda=lr_schedule, step_every='batch'),\n",
    "            ProgressBar(),\n",
    "        ],\n",
    "    )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HuggingfacePretrainedTokenizerWithY.transform() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/scratch/lgrinszt/lm_tab/scripts/launch.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/lm_tab/scripts/launch.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m HuggingfacePretrainedTokenizerWithY(TOKENIZER)\u001b[39m.\u001b[39;49mfit_transform(X[:\u001b[39m10\u001b[39;49m], y[:\u001b[39m10\u001b[39;49m])\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/base.py:919\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    917\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 919\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: HuggingfacePretrainedTokenizerWithY.transform() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "HuggingfacePretrainedTokenizerWithY(TOKENIZER).fit_transform(X[:10], y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79eb72f26fee4429856641cb68d3d206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_params {}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/scratch/lgrinszt/lm_tab/scripts/launch.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/lm_tab/scripts/launch.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit(X, y)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    429\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/classifier.py:165\u001b[0m, in \u001b[0;36mNeuralNetClassifier.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[39mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m# this is actually a pylint bug:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(NeuralNetClassifier, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1319\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialized_:\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialize()\n\u001b[0;32m-> 1319\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1320\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1278\u001b[0m, in \u001b[0;36mNeuralNet.partial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m'\u001b[39m\u001b[39mon_train_begin\u001b[39m\u001b[39m'\u001b[39m, X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my)\n\u001b[1;32m   1277\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1279\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1190\u001b[0m, in \u001b[0;36mNeuralNet.fit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m   1188\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m'\u001b[39m\u001b[39mon_epoch_begin\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mon_epoch_kwargs)\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single_epoch(iterator_train, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1191\u001b[0m                           step_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single_epoch(iterator_valid, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1194\u001b[0m                           step_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_step, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m   1196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m\"\u001b[39m\u001b[39mon_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mon_epoch_kwargs)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1226\u001b[0m, in \u001b[0;36mNeuralNet.run_single_epoch\u001b[0;34m(self, iterator, training, prefix, step_fn, **fit_params)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m   1225\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m\"\u001b[39m\u001b[39mon_batch_begin\u001b[39m\u001b[39m\"\u001b[39m, batch\u001b[39m=\u001b[39mbatch, training\u001b[39m=\u001b[39mtraining)\n\u001b[0;32m-> 1226\u001b[0m     step \u001b[39m=\u001b[39m step_fn(batch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1227\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mrecord_batch(prefix \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_loss\u001b[39m\u001b[39m\"\u001b[39m, step[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem())\n\u001b[1;32m   1228\u001b[0m     batch_size \u001b[39m=\u001b[39m (get_len(batch[\u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(batch, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m))\n\u001b[1;32m   1229\u001b[0m                   \u001b[39melse\u001b[39;00m get_len(batch))\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1105\u001b[0m, in \u001b[0;36mNeuralNet.train_step\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\n\u001b[1;32m   1098\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mon_grad_computed\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1099\u001b[0m         named_parameters\u001b[39m=\u001b[39mTeeGenerator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_all_learnable_params()),\n\u001b[1;32m   1100\u001b[0m         batch\u001b[39m=\u001b[39mbatch,\n\u001b[1;32m   1101\u001b[0m         training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[1;32m   1103\u001b[0m     \u001b[39mreturn\u001b[39;00m step[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m-> 1105\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_optimizer(step_fn)\n\u001b[1;32m   1106\u001b[0m \u001b[39mreturn\u001b[39;00m step_accumulator\u001b[39m.\u001b[39mget_step()\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1060\u001b[0m, in \u001b[0;36mNeuralNet._step_optimizer\u001b[0;34m(self, step_fn)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m   1059\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(step_fn)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/optim/adamw.py:148\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 148\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    150\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    151\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1094\u001b[0m, in \u001b[0;36mNeuralNet.train_step.<locals>.step_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_fn\u001b[39m():\n\u001b[1;32m   1093\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_optimizer()\n\u001b[0;32m-> 1094\u001b[0m     step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step_single(batch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1095\u001b[0m     step_accumulator\u001b[39m.\u001b[39mstore_step(step)\n\u001b[1;32m   1097\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\n\u001b[1;32m   1098\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mon_grad_computed\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1099\u001b[0m         named_parameters\u001b[39m=\u001b[39mTeeGenerator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_all_learnable_params()),\n\u001b[1;32m   1100\u001b[0m         batch\u001b[39m=\u001b[39mbatch,\n\u001b[1;32m   1101\u001b[0m         training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1102\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:993\u001b[0m, in \u001b[0;36mNeuralNet.train_step_single\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_training(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    992\u001b[0m Xi, yi \u001b[39m=\u001b[39m unpack_data(batch)\n\u001b[0;32m--> 993\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(Xi, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    994\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_loss(y_pred, yi, X\u001b[39m=\u001b[39mXi, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    995\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1520\u001b[0m, in \u001b[0;36mNeuralNet.infer\u001b[0;34m(self, x, **fit_params)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, Mapping):\n\u001b[1;32m   1519\u001b[0m     x_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_x_and_fit_params(x, fit_params)\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule_(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mx_dict)\n\u001b[1;32m   1521\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/lgrinszt/lm_tab/src/models.py:34\u001b[0m, in \u001b[0;36mBertAndTabPFN.forward\u001b[0;34m(self, input_ids, attention_mask, tabular_data, single_eval_pos, **fit_params)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_before_tabpfn:\n\u001b[1;32m     33\u001b[0m     tabpfn_input \u001b[39m=\u001b[39m preprocess_input(tabpfn_input, y, single_eval_pos, preprocess_transform\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39minput_ids\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 34\u001b[0m tabpfn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtabpfn((tabpfn_input, y), single_eval_pos\u001b[39m=\u001b[39msingle_eval_pos)\n\u001b[1;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m tabpfn_outputs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1303619598.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    from sklea\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklea\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # return self for compatibility reasons\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Your code here that uses both X and y\n",
    "        print(\"X\", X.shape)\n",
    "        print(\"y\", y.shape)\n",
    "        return X\n",
    "    \n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('custom_transformer', CustomTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/scratch/lgrinszt/lm_tab/scripts/launch.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/lm_tab/scripts/launch.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m X, y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m100\u001b[39m, \u001b[39m10\u001b[39m), np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m100\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/lm_tab/scripts/launch.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit(X, y)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    429\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/svm/_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    191\u001b[0m         X,\n\u001b[1;32m    192\u001b[0m         y,\n\u001b[1;32m    193\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[1;32m    194\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    195\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    196\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[1;32m    199\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/utils/validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[0;32m-> 1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1147\u001b[0m     X,\n\u001b[1;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1149\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1150\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1151\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1152\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1153\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1154\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1155\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1156\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1157\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1158\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[1;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/utils/validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    914\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    916\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    918\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    919\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'ellipsis'"
     ]
    }
   ],
   "source": [
    "X, y = np.random.rand(100, 10), np.random.randint(0, 2, 100)\n",
    "\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
