{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loading import load_data\n",
    "from skrub import MinHashEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from src.utils import FeaturesExtractor, FixedSizeSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(X, encoder_name):\n",
    "    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "        X = np.array(X)\n",
    "    encoder_type, encoder_params = encoder_name.split(\"__\", 1)\n",
    "    if encoder_type == \"lm\":\n",
    "        encoder = SentenceTransformer(encoder_params)\n",
    "        return encoder.encode(X)\n",
    "    elif encoder_type == \"skrub\":\n",
    "        if encoder_params.startswith(\"minhash\"):\n",
    "            n_components = int(encoder_params.split(\"_\")[1])\n",
    "            encoder = MinHashEncoder(n_components=n_components)\n",
    "            # reshape to 2d array\n",
    "            # if pandas dataframe, convert to numpy array\n",
    "            X = X.reshape(-1, 1)\n",
    "            return encoder.fit_transform(X)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown skrub encoder {encoder_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class FeaturesExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_features=1, method=\"first\"):\n",
    "        self.n_features = n_features\n",
    "        self.method = method\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Nothing to fit, so return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Extract the first n_features\n",
    "        # choose features to keep\n",
    "        if self.method == \"first\":\n",
    "            res = X[:, :self.n_features]\n",
    "        elif self.method == \"last\":\n",
    "            res = X[:, -self.n_features:]\n",
    "        elif self.method == \"middle\":\n",
    "            res = X[:, self.n_features//2:self.n_features//2+self.n_features]\n",
    "        elif self.method == \"random\":\n",
    "            res = X[:, np.random.choice(X.shape[1], self.n_features, replace=False)]\n",
    "        elif self.method == \"biggest_variance\":\n",
    "            indices = np.argsort(np.var(X, axis=0))[-self.n_features:]\n",
    "            res = X[:, indices]\n",
    "        elif self.method == \"smallest_variance\":\n",
    "            indices = np.argsort(np.var(X, axis=0))[:self.n_features]\n",
    "            res = X[:, indices]\n",
    "        \n",
    "        assert res.shape == (X.shape[0], self.n_features)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_data() got an unexpected keyword argument 'include_all_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m load_data(\u001b[39m\"\u001b[39;49m\u001b[39mspotify\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_rows\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, include_all_columns\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: load_data() got an unexpected keyword argument 'include_all_columns'"
     ]
    }
   ],
   "source": [
    "load_data(\"spotify\", max_rows=10000, include_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Number of iterations:  36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: journal_jcr_cls, Encoding: lm__all-distilroberta-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_data() got an unexpected keyword argument 'include_all_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X51sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m tqdm(encodings, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X51sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39m#TODO do a proper sklearn transform (not a problem for sentence transformers)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X51sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39m# right now not done for speed reasons and gpu handling\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X51sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataset: \u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m}\u001b[39;00m\u001b[39m, Encoding: \u001b[39m\u001b[39m{\u001b[39;00mencoding\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X51sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     X_text, X_rest, y \u001b[39m=\u001b[39m load_data(dataset, max_rows\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, include_all_columns\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X51sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39m# encode X_text\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X51sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     X_enc \u001b[39m=\u001b[39m encode(X_text, encoding)\n",
      "\u001b[0;31mTypeError\u001b[0m: load_data() got an unexpected keyword argument 'include_all_columns'"
     ]
    }
   ],
   "source": [
    "#encodings = [\"skrub__minhash_10\", \"skrub__minhash_30\", \"lm__all-MiniLM-L12-v2\", \"lm__whaleloops/phrase-bert\"]\n",
    "encodings = [\"lm__all-distilroberta-v1\"]\n",
    "datasets = [\"journal_jcr_cls\", \"movies\", \"michelin\", \"spotify\", \"employee_salary\", \"drug_directory\", \"museums\", \"fifa_footballplayers_22\", \"jp_anime\"]\n",
    "#dim_reductions = {\"PCA_10\": PCA(n_components=10), \"PCA_30\": PCA(n_components=10), \"subset_10\": FeaturesExtractor(method=\"first\", n_features=10), \"subset_30\": FeaturesExtractor(method=\"first\", n_features=30),\n",
    "#                  \"subset_biggest_10\": FeaturesExtractor(method=\"biggest_variance\", n_features=10), \"subset_biggest_30\": FeaturesExtractor(method=\"biggest_variance\", n_features=30),\n",
    "#                  \"passthrough\": \"passthrough\"}\n",
    "#models = {\"LogisticRegression\": LogisticRegression(), \"GradientBoostingClassifier\": GradientBoostingClassifier(), \"TabPFNClassifier\": TabPFNClassifier(device=\"cpu\")}\n",
    "dim_reductions = {\"subset_100\": FeaturesExtractor(method=\"first\", n_features=100),\n",
    "                  \"subset_smallest_50\": FeaturesExtractor(method=\"smallest_variance\", n_features=50),\n",
    "                  \"subset_smallest_100\": FeaturesExtractor(method=\"smallest_variance\", n_features=100),\n",
    "                  \"PCA_100\": PCA(n_components=100)}\n",
    "\n",
    "models = {\"TabPFNClassifier\": TabPFNClassifier(device=\"cpu\")}\n",
    "print(\"Number of iterations: \", len(datasets) * len(encodings) * len(dim_reductions) * len(models))\n",
    "\n",
    "def run_on_encoded_data(X_enc, X_rest, y, dim_reduction_name, dim_reduction, model_name, model):\n",
    "    if dim_reduction_name == \"passthrough\" and model_name != \"LogisticRegression\" and not encoding.startswith(\"skrub\"):\n",
    "        return None\n",
    "    if dim_reduction_name != \"passthrough\" and encoding.startswith(\"skrub\"):\n",
    "        return None\n",
    "    # encode X_rest with the TableVectorizer\n",
    "    if model_name == \"TabPFNClassifier\":\n",
    "        # ordinal encoding for low_cardinality columns\n",
    "        low_card_cat_transformer = OrdinalEncoder()\n",
    "    else:\n",
    "        low_card_cat_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    if model_name == \"LogisticRegression\":\n",
    "        numerical_transformer = StandardScaler()\n",
    "    else:\n",
    "        numerical_transformer = \"passthrough\"\n",
    "    \n",
    "    rest_trans = TableVectorizer(high_card_cat_transformer=MinHashEncoder(),\n",
    "                                low_card_cat_transformer=low_card_cat_transformer,\n",
    "                                numerical_transformer=numerical_transformer)\n",
    "    \n",
    "    # Assuming X_enc and X_rest are numpy arrays, you can get their shapes\n",
    "    n_enc_columns = X_enc.shape[1]\n",
    "    n_rest_columns = X_rest.shape[1]\n",
    "\n",
    "    # Create column indices for X_enc and X_rest\n",
    "    enc_indices = np.arange(n_enc_columns)\n",
    "    rest_indices = np.arange(n_enc_columns, n_enc_columns + n_rest_columns)\n",
    "\n",
    "    # Create the ColumnTransformer\n",
    "    complete_trans = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('dim_reduction', dim_reduction, enc_indices),  # Apply dimensionality reduction to X_enc\n",
    "            ('rest_trans', rest_trans, rest_indices)  # Apply TableVectorizer to X_rest\n",
    "        ])\n",
    "    \n",
    "\n",
    "    full_X = np.concatenate([X_enc, X_rest], axis=1)\n",
    "\n",
    "\n",
    "    pipeline = Pipeline([(\"encoding\", complete_trans), (\"model\", model)])\n",
    "    cv = FixedSizeSplit(n_splits=5, n_train=1000, n_test=4000)\n",
    "    scores = cross_val_score(pipeline, full_X, y, scoring=\"accuracy\", cv=cv)\n",
    "    return scores\n",
    "\n",
    "results = pd.DataFrame(columns=[\"dataset\", \"encoding\", \"dim_reduction\", \"model\", \"accuracy\"])\n",
    "\n",
    "for dataset in tqdm(datasets):\n",
    "    for encoding in tqdm(encodings, leave=False):\n",
    "        #TODO do a proper sklearn transform (not a problem for sentence transformers)\n",
    "        # right now not done for speed reasons and gpu handling\n",
    "        print(f\"Dataset: {dataset}, Encoding: {encoding}\")\n",
    "        X_text, X_rest, y = load_data(dataset, max_rows=10000, include_all_columns=True)\n",
    "        # encode X_text\n",
    "        X_enc = encode(X_text, encoding)\n",
    "\n",
    "\n",
    "\n",
    "        # # run with joblib\n",
    "        results_data_enc = Parallel(n_jobs=-1)(delayed(run_on_encoded_data)(X_enc, X_rest, y, dim_reduction_name, dim_reduction,model_name, model) for (dim_reduction_name, dim_reduction) in dim_reductions.items() for (model_name, model) in models.items())\n",
    "        start_time = time.time()\n",
    "        for dim_reduction_name, dim_reduction in dim_reductions.items():\n",
    "            for model_name, model in models.items():\n",
    "                scores = results_data_enc.pop(0)\n",
    "                if scores is None:\n",
    "                    continue\n",
    "                results = pd.concat([results, pd.DataFrame({\"dataset\": dataset, \"encoding\": encoding, \"dim_reduction\": dim_reduction_name, \"model\": model_name, \"accuracy\": scores})])\n",
    "        print(f\"Time elapsed: {time.time() - start_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "#results.to_csv(\"results.csv\", index=False)\n",
    "results.to_csv(\"results2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>encoding</th>\n",
       "      <th>dim_reduction</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>journal_jcr_cls</td>\n",
       "      <td>lm__all-distilroberta-v1</td>\n",
       "      <td>subset_100</td>\n",
       "      <td>TabPFNClassifier</td>\n",
       "      <td>0.58700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>journal_jcr_cls</td>\n",
       "      <td>lm__all-distilroberta-v1</td>\n",
       "      <td>subset_100</td>\n",
       "      <td>TabPFNClassifier</td>\n",
       "      <td>0.58875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>journal_jcr_cls</td>\n",
       "      <td>lm__all-distilroberta-v1</td>\n",
       "      <td>subset_100</td>\n",
       "      <td>TabPFNClassifier</td>\n",
       "      <td>0.58350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>journal_jcr_cls</td>\n",
       "      <td>lm__all-distilroberta-v1</td>\n",
       "      <td>subset_100</td>\n",
       "      <td>TabPFNClassifier</td>\n",
       "      <td>0.58575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>journal_jcr_cls</td>\n",
       "      <td>lm__all-distilroberta-v1</td>\n",
       "      <td>subset_100</td>\n",
       "      <td>TabPFNClassifier</td>\n",
       "      <td>0.59400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jp_anime</td>\n",
       "      <td>lm__all-distilroberta-v1</td>\n",
       "      <td>PCA_100</td>\n",
       "      <td>TabPFNClassifier</td>\n",
       "      <td>0.58375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jp_anime</td>\n",
       "      <td>lm__all-distilroberta-v1</td>\n",
       "      <td>PCA_100</td>\n",
       "      <td>TabPFNClassifier</td>\n",
       "      <td>0.58775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jp_anime</td>\n",
       "      <td>lm__all-distilroberta-v1</td>\n",
       "      <td>PCA_100</td>\n",
       "      <td>TabPFNClassifier</td>\n",
       "      <td>0.56575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jp_anime</td>\n",
       "      <td>lm__all-distilroberta-v1</td>\n",
       "      <td>PCA_100</td>\n",
       "      <td>TabPFNClassifier</td>\n",
       "      <td>0.57850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jp_anime</td>\n",
       "      <td>lm__all-distilroberta-v1</td>\n",
       "      <td>PCA_100</td>\n",
       "      <td>TabPFNClassifier</td>\n",
       "      <td>0.56225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dataset                  encoding dim_reduction             model  \\\n",
       "0   journal_jcr_cls  lm__all-distilroberta-v1    subset_100  TabPFNClassifier   \n",
       "1   journal_jcr_cls  lm__all-distilroberta-v1    subset_100  TabPFNClassifier   \n",
       "2   journal_jcr_cls  lm__all-distilroberta-v1    subset_100  TabPFNClassifier   \n",
       "3   journal_jcr_cls  lm__all-distilroberta-v1    subset_100  TabPFNClassifier   \n",
       "4   journal_jcr_cls  lm__all-distilroberta-v1    subset_100  TabPFNClassifier   \n",
       "..              ...                       ...           ...               ...   \n",
       "0          jp_anime  lm__all-distilroberta-v1       PCA_100  TabPFNClassifier   \n",
       "1          jp_anime  lm__all-distilroberta-v1       PCA_100  TabPFNClassifier   \n",
       "2          jp_anime  lm__all-distilroberta-v1       PCA_100  TabPFNClassifier   \n",
       "3          jp_anime  lm__all-distilroberta-v1       PCA_100  TabPFNClassifier   \n",
       "4          jp_anime  lm__all-distilroberta-v1       PCA_100  TabPFNClassifier   \n",
       "\n",
       "    accuracy  \n",
       "0    0.58700  \n",
       "1    0.58875  \n",
       "2    0.58350  \n",
       "3    0.58575  \n",
       "4    0.59400  \n",
       "..       ...  \n",
       "0    0.58375  \n",
       "1    0.58775  \n",
       "2    0.56575  \n",
       "3    0.57850  \n",
       "4    0.56225  \n",
       "\n",
       "[180 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/lm_tab/scripts\n"
     ]
    }
   ],
   "source": [
    "%cd lm_tab/scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "boxpoints": "all",
         "customdata": [],
         "fillcolor": "rgba(255,255,255,0)",
         "hoveron": "points",
         "hovertemplate": "dataset: %{customdata[0]}<br>model: %{customdata[1]}<br>dim_reduction: %{customdata[2]}<br>encoding: %{customdata[3]}<br>accuracy: %{customdata[4]}",
         "legendgroup": "PCA_100",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "marker": {
          "color": "#636efa"
         },
         "name": "PCA_100",
         "offsetgroup": "PCA_100",
         "orientation": "h",
         "pointpos": 0,
         "showlegend": true,
         "type": "box",
         "x": [
          0.8843500000000001,
          0.8497999999999999,
          0.567,
          0.59475,
          0.5756,
          0.6089,
          0.55035,
          0.6335,
          0.75115
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          "drug_directory",
          "employee_salary",
          "fifa_footballplayers_22",
          "journal_jcr_cls",
          "jp_anime",
          "michelin",
          "movies",
          "museums",
          "spotify"
         ],
         "y0": " ",
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "boxpoints": "all",
         "customdata": [],
         "fillcolor": "rgba(255,255,255,0)",
         "hoveron": "points",
         "hovertemplate": "dataset: %{customdata[0]}<br>model: %{customdata[1]}<br>dim_reduction: %{customdata[2]}<br>encoding: %{customdata[3]}<br>accuracy: %{customdata[4]}",
         "legendgroup": "subset_100",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "marker": {
          "color": "#EF553B"
         },
         "name": "subset_100",
         "offsetgroup": "subset_100",
         "orientation": "h",
         "pointpos": 0,
         "showlegend": true,
         "type": "box",
         "x": [
          0.88355,
          0.8516,
          0.5854,
          0.5878,
          0.56835,
          0.59245,
          0.53935,
          0.63645,
          0.7352000000000001
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          "drug_directory",
          "employee_salary",
          "fifa_footballplayers_22",
          "journal_jcr_cls",
          "jp_anime",
          "michelin",
          "movies",
          "museums",
          "spotify"
         ],
         "y0": " ",
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "boxpoints": "all",
         "customdata": [],
         "fillcolor": "rgba(255,255,255,0)",
         "hoveron": "points",
         "hovertemplate": "dataset: %{customdata[0]}<br>model: %{customdata[1]}<br>dim_reduction: %{customdata[2]}<br>encoding: %{customdata[3]}<br>accuracy: %{customdata[4]}",
         "legendgroup": "subset_smallest_100",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "marker": {
          "color": "#00cc96"
         },
         "name": "subset_smallest_100",
         "offsetgroup": "subset_smallest_100",
         "orientation": "h",
         "pointpos": 0,
         "showlegend": true,
         "type": "box",
         "x": [
          0.583,
          0.57875,
          0.49824999999999997,
          0.53345,
          0.4992,
          0.51795,
          0.4953,
          0.5094,
          0.5244500000000001
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          "drug_directory",
          "employee_salary",
          "fifa_footballplayers_22",
          "journal_jcr_cls",
          "jp_anime",
          "michelin",
          "movies",
          "museums",
          "spotify"
         ],
         "y0": " ",
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "boxpoints": "all",
         "customdata": [],
         "fillcolor": "rgba(255,255,255,0)",
         "hoveron": "points",
         "hovertemplate": "dataset: %{customdata[0]}<br>model: %{customdata[1]}<br>dim_reduction: %{customdata[2]}<br>encoding: %{customdata[3]}<br>accuracy: %{customdata[4]}",
         "legendgroup": "subset_smallest_50",
         "line": {
          "color": "rgba(255,255,255,0)"
         },
         "marker": {
          "color": "#ab63fa"
         },
         "name": "subset_smallest_50",
         "offsetgroup": "subset_smallest_50",
         "orientation": "h",
         "pointpos": 0,
         "showlegend": true,
         "type": "box",
         "x": [
          0.50335,
          0.5433,
          0.50885,
          0.53255,
          0.48865,
          0.50705,
          0.49779999999999996,
          0.5197499999999999,
          0.53655
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          "drug_directory",
          "employee_salary",
          "fifa_footballplayers_22",
          "journal_jcr_cls",
          "jp_anime",
          "michelin",
          "movies",
          "museums",
          "spotify"
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "height": 600,
        "legend": {
         "title": {
          "text": "dim_reduction"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Swarm Plot of Model Accuracies Across Datasets"
        },
        "width": 900,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Accuracy"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Dataset"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"5ddf90ea-5ef2-45ff-9256-9b24501cca35\" class=\"plotly-graph-div\" style=\"height:600px; width:900px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5ddf90ea-5ef2-45ff-9256-9b24501cca35\")) {                    Plotly.newPlot(                        \"5ddf90ea-5ef2-45ff-9256-9b24501cca35\",                        [{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"fillcolor\":\"rgba(255,255,255,0)\",\"hoveron\":\"points\",\"hovertemplate\":\"dataset: %{customdata[0]}\\u003cbr\\u003emodel: %{customdata[1]}\\u003cbr\\u003edim_reduction: %{customdata[2]}\\u003cbr\\u003eencoding: %{customdata[3]}\\u003cbr\\u003eaccuracy: %{customdata[4]}\",\"legendgroup\":\"PCA_100\",\"line\":{\"color\":\"rgba(255,255,255,0)\"},\"marker\":{\"color\":\"#636efa\"},\"name\":\"PCA_100\",\"offsetgroup\":\"PCA_100\",\"orientation\":\"h\",\"pointpos\":0,\"showlegend\":true,\"x\":[0.8843500000000001,0.8497999999999999,0.567,0.59475,0.5756,0.6089,0.55035,0.6335,0.75115],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[\"drug_directory\",\"employee_salary\",\"fifa_footballplayers_22\",\"journal_jcr_cls\",\"jp_anime\",\"michelin\",\"movies\",\"museums\",\"spotify\"],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\",\"customdata\":[]},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"fillcolor\":\"rgba(255,255,255,0)\",\"hoveron\":\"points\",\"hovertemplate\":\"dataset: %{customdata[0]}\\u003cbr\\u003emodel: %{customdata[1]}\\u003cbr\\u003edim_reduction: %{customdata[2]}\\u003cbr\\u003eencoding: %{customdata[3]}\\u003cbr\\u003eaccuracy: %{customdata[4]}\",\"legendgroup\":\"subset_100\",\"line\":{\"color\":\"rgba(255,255,255,0)\"},\"marker\":{\"color\":\"#EF553B\"},\"name\":\"subset_100\",\"offsetgroup\":\"subset_100\",\"orientation\":\"h\",\"pointpos\":0,\"showlegend\":true,\"x\":[0.88355,0.8516,0.5854,0.5878,0.56835,0.59245,0.53935,0.63645,0.7352000000000001],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[\"drug_directory\",\"employee_salary\",\"fifa_footballplayers_22\",\"journal_jcr_cls\",\"jp_anime\",\"michelin\",\"movies\",\"museums\",\"spotify\"],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\",\"customdata\":[]},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"fillcolor\":\"rgba(255,255,255,0)\",\"hoveron\":\"points\",\"hovertemplate\":\"dataset: %{customdata[0]}\\u003cbr\\u003emodel: %{customdata[1]}\\u003cbr\\u003edim_reduction: %{customdata[2]}\\u003cbr\\u003eencoding: %{customdata[3]}\\u003cbr\\u003eaccuracy: %{customdata[4]}\",\"legendgroup\":\"subset_smallest_100\",\"line\":{\"color\":\"rgba(255,255,255,0)\"},\"marker\":{\"color\":\"#00cc96\"},\"name\":\"subset_smallest_100\",\"offsetgroup\":\"subset_smallest_100\",\"orientation\":\"h\",\"pointpos\":0,\"showlegend\":true,\"x\":[0.583,0.57875,0.49824999999999997,0.53345,0.4992,0.51795,0.4953,0.5094,0.5244500000000001],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[\"drug_directory\",\"employee_salary\",\"fifa_footballplayers_22\",\"journal_jcr_cls\",\"jp_anime\",\"michelin\",\"movies\",\"museums\",\"spotify\"],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\",\"customdata\":[]},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"fillcolor\":\"rgba(255,255,255,0)\",\"hoveron\":\"points\",\"hovertemplate\":\"dataset: %{customdata[0]}\\u003cbr\\u003emodel: %{customdata[1]}\\u003cbr\\u003edim_reduction: %{customdata[2]}\\u003cbr\\u003eencoding: %{customdata[3]}\\u003cbr\\u003eaccuracy: %{customdata[4]}\",\"legendgroup\":\"subset_smallest_50\",\"line\":{\"color\":\"rgba(255,255,255,0)\"},\"marker\":{\"color\":\"#ab63fa\"},\"name\":\"subset_smallest_50\",\"offsetgroup\":\"subset_smallest_50\",\"orientation\":\"h\",\"pointpos\":0,\"showlegend\":true,\"x\":[0.50335,0.5433,0.50885,0.53255,0.48865,0.50705,0.49779999999999996,0.5197499999999999,0.53655],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[\"drug_directory\",\"employee_salary\",\"fifa_footballplayers_22\",\"journal_jcr_cls\",\"jp_anime\",\"michelin\",\"movies\",\"museums\",\"spotify\"],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\",\"customdata\":[]}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Accuracy\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Dataset\"}},\"legend\":{\"title\":{\"text\":\"dim_reduction\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Swarm Plot of Model Accuracies Across Datasets\"},\"boxmode\":\"group\",\"height\":600,\"width\":900},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5ddf90ea-5ef2-45ff-9256-9b24501cca35');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "#results = pd.read_csv(\"../results/results.csv\")\n",
    "#esults['mean_accuracy'] = results['accuracy'].apply(np.mean)\n",
    "#melted_results = results\n",
    "# \"Melting\" the data to make it suitable for a swarmplot\n",
    "#melted_results = results.explode('accuracy')\n",
    "#melted_results['accuracy'] = melted_results['accuracy'].astype(float)\n",
    "# average the accuracy scores in the accuracy column\n",
    "\n",
    "# filter for encoding =lm__all-MiniLM-L12-v2\n",
    "# take the mean of accuracy\n",
    "melted_results = results.groupby(['dataset', 'model', 'dim_reduction', 'encoding']).mean().reset_index()\n",
    "#melted_results = results.explode('accuracy')\n",
    "#melted_results['accuracy'] = melted_results['accuracy'].astype(float)\n",
    "#melted_results = melted_results[melted_results['encoding'] == 'lm__all-MiniLM-L12-v2']\n",
    "print(len(melted_results))\n",
    "\n",
    "# Creating the swarmplot\n",
    "# plt.figure(figsize=(15, 20))\n",
    "# sns.swarmplot(data=melted_results, x='accuracy', y='dataset', hue='model', dodge=True)\n",
    "# plt.title('Swarm Plot of Model Accuracies Across Datasets')\n",
    "# plt.xlabel('Accuracy')\n",
    "# plt.ylabel('Dataset')\n",
    "# plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# plt.show()\n",
    "# Create the plot\n",
    "\n",
    "# Create the plot\n",
    "fig = px.strip(\n",
    "    data_frame=melted_results,\n",
    "    x=\"accuracy\",\n",
    "    y=\"dataset\",\n",
    "    #color=\"model\",\n",
    "    color=\"dim_reduction\",\n",
    "    title=\"Swarm Plot of Model Accuracies Across Datasets\",\n",
    "    labels={\"accuracy\": \"Accuracy\", \"dataset\": \"Dataset\", \"model\": \"Model\"},\n",
    "    height=600,\n",
    "    width=900,\n",
    ")\n",
    "\n",
    "# Update hover information for each trace (grouped by 'color' or 'model' in this case)\n",
    "for i, trace in enumerate(fig.data):\n",
    "    subset_df = melted_results[melted_results['model'] == trace.name]\n",
    "    hover_template = \"<br>\".join([f\"{col}: %{{customdata[{i}]}}\" for i, col in enumerate(subset_df.columns)])\n",
    "    trace.customdata = subset_df.values\n",
    "    trace.hovertemplate = hover_template\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/lm_tab/scripts\n"
     ]
    }
   ],
   "source": [
    "%cd lm_tab/scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/lm_tab/scripts\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original task: classification for spotify\n"
     ]
    }
   ],
   "source": [
    "# compare speed on cpu and gpu\n",
    "X, y = load_data(\"spotify\", max_rows=10000)\n",
    "# label encoding\n",
    "#y = y.astype('category').cat.codes\n",
    "y = y.astype(np.int64)\n",
    "#X_enc = encode(X, \"lm__all-MiniLM-L12-v2\")\n",
    "X, y = X[:500], y[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a tokenizer and BERT model that work together\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import LRScheduler, ProgressBar\n",
    "from skorch.hf import HuggingfacePretrainedTokenizer\n",
    "# import LambdaLR from torch.optim.lr_scheduler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from src.models import BertAndTabPFN\n",
    "\n",
    "#TOKENIZER = \"distilbert-base-uncased\"\n",
    "#PRETRAINED_MODEL = \"distilbert-base-uncased\"\n",
    "TOKENIZER = 'distilroberta-base'\n",
    "PRETRAINED_MODEL = 'distilroberta-base'\n",
    "\n",
    "# model hyper-parameters\n",
    "OPTMIZER = torch.optim.AdamW\n",
    "LR = 5e-5\n",
    "MAX_EPOCHS = 3\n",
    "CRITERION = nn.CrossEntropyLoss\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "num_training_steps = MAX_EPOCHS * (len(X) // BATCH_SIZE + 1)\n",
    "\n",
    "def lr_schedule(current_step):\n",
    "    factor = float(num_training_steps - current_step) / float(max(1, num_training_steps))\n",
    "    assert factor > 0\n",
    "    return factor\n",
    "\n",
    "class BertModule(nn.Module):\n",
    "    def __init__(self, name, num_labels):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.reset_weights()\n",
    "        \n",
    "    def reset_weights(self):\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.name, num_labels=self.num_labels\n",
    "        )\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        pred = self.bert(**kwargs)\n",
    "        return pred.logits\n",
    "    \n",
    "class BertAndTabPFNWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, X, **fit_params):\n",
    "        y = fit_params.get(\"y\", None)\n",
    "        return self.model(X, y)\n",
    "    \n",
    "#wrapped_model = BertAndTabPFNWrapper(BertAndTabPFN)\n",
    "\n",
    "class HuggingfacePretrainedTokenizerWithY(HuggingfacePretrainedTokenizer):\n",
    "    def transform(self, X):\n",
    "        res = super().transform(X)\n",
    "        res[\"y\"] = y\n",
    "        return res\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tokenizer', HuggingfacePretrainedTokenizer(TOKENIZER)),\n",
    "    ('net', NeuralNetClassifier(\n",
    "        BertAndTabPFN,\n",
    "        module__dim_tabpfn=50, \n",
    "        module__preprocess_before_tabpfn=False,\n",
    "        #module__name=PRETRAINED_MODEL,\n",
    "        #module__num_labels=len(set(y)),\n",
    "        optimizer=OPTMIZER,\n",
    "        lr=LR,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        criterion=CRITERION,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        iterator_train__shuffle=True,\n",
    "        device=DEVICE,\n",
    "        callbacks=[\n",
    "            LRScheduler(LambdaLR, lr_lambda=lr_schedule, step_every='batch'),\n",
    "            ProgressBar(),\n",
    "        ],\n",
    "    )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = HuggingfacePretrainedTokenizer(TOKENIZER).fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MergeDictTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Make a copy of X to avoid modifying the original dictionary\n",
    "        merged_dict = X.copy()\n",
    "        # Merge y into the copied dictionary\n",
    "        if y is not None:\n",
    "            merged_dict.update({\"y\": y.astype(np.float32)})\n",
    "            \n",
    "        return merged_dict\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X, y)\n",
    "    \n",
    "step2 = MergeDictTransformer().fit_transform(step1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2[\"y\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from tabpfn import TabPFNClassifier\n",
    "from src.utils import preprocess_input\n",
    "from transformers import AutoModel\n",
    "class BertAndTabPFN(nn.Module):\n",
    "    def __init__(self, linear_translator=False, dim_tabpfn=100, preprocess_before_tabpfn=False,\n",
    "                 train_tabpfn=False):\n",
    "        super().__init__()\n",
    "        #self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        #self.bert = BertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.bert = AutoModel.from_pretrained('distilroberta-base')\n",
    "        self.tabpfn = TabPFNClassifier().model[2]\n",
    "        if not train_tabpfn:\n",
    "            # no requires_grad for the tabpfn\n",
    "            for param in self.tabpfn.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.dim_tabpfn = dim_tabpfn\n",
    "        self.preprocess_before_tabpfn = preprocess_before_tabpfn\n",
    "        if linear_translator:\n",
    "            self.linear_translator = nn.Linear(768, dim_tabpfn)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, y, tabular_data=None, single_eval_pos=100, **fit_params):\n",
    "        bert_outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        bert_embeddings = bert_outputs.last_hidden_state[:, 0, :]\n",
    "        if hasattr(self, 'linear_translator'):\n",
    "            tabpfn_input = self.linear_translator(bert_embeddings)\n",
    "        else:\n",
    "            tabpfn_input = bert_embeddings[:, :self.dim_tabpfn]\n",
    "        tabpfn_input = tabpfn_input.reshape(tabpfn_input.shape[0], 1, tabpfn_input.shape[1])\n",
    "        if self.preprocess_before_tabpfn:\n",
    "            tabpfn_input = preprocess_input(tabpfn_input, y, single_eval_pos, preprocess_transform=\"none\", device=input_ids.device)\n",
    "        # print shapes\n",
    "        print(\"bert_embeddings.shape\", bert_embeddings.shape)\n",
    "        print(\"tabpfn_input.shape\", tabpfn_input.shape)\n",
    "        print(\"y.shape\", y.shape)\n",
    "        y = y.reshape(y.shape[0], 1)\n",
    "        tabpfn_outputs = self.tabpfn((tabpfn_input, y), single_eval_pos=single_eval_pos)\n",
    "        return tabpfn_outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step2_cuda = {k: torch.tensor(v).to(\"cuda\")[:200] for k, v in step2.items()}\n",
    "#model = BertAndTabPFN().to('cuda')\n",
    "#model(**step2_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NeuralNetClassifier(\n",
    "        BertAndTabPFN,\n",
    "        module__dim_tabpfn=100, \n",
    "        module__preprocess_before_tabpfn=False,\n",
    "        #module__name=PRETRAINED_MODEL,\n",
    "        #module__num_labels=len(set(y)),\n",
    "        optimizer=OPTMIZER,\n",
    "        lr=LR,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        criterion=CRITERION,\n",
    "        batch_size=200,\n",
    "        iterator_train__shuffle=True,\n",
    "        device=DEVICE,\n",
    "        callbacks=[\n",
    "            LRScheduler(LambdaLR, lr_lambda=lr_schedule, step_every='batch'),\n",
    "            ProgressBar(),\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb18f4fe1fdb46afb656ecd55283a193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_embeddings.shape torch.Size([200, 768])\n",
      "tabpfn_input.shape torch.Size([200, 1, 100])\n",
      "y.shape torch.Size([200])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (100) to match target batch_size (200).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(step2, y)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/classifier.py:165\u001b[0m, in \u001b[0;36mNeuralNetClassifier.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[39mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m# this is actually a pylint bug:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(NeuralNetClassifier, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1319\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialized_:\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialize()\n\u001b[0;32m-> 1319\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1320\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1278\u001b[0m, in \u001b[0;36mNeuralNet.partial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m'\u001b[39m\u001b[39mon_train_begin\u001b[39m\u001b[39m'\u001b[39m, X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my)\n\u001b[1;32m   1277\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1279\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1190\u001b[0m, in \u001b[0;36mNeuralNet.fit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m   1188\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m'\u001b[39m\u001b[39mon_epoch_begin\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mon_epoch_kwargs)\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single_epoch(iterator_train, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1191\u001b[0m                           step_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single_epoch(iterator_valid, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1194\u001b[0m                           step_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_step, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m   1196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m\"\u001b[39m\u001b[39mon_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mon_epoch_kwargs)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1226\u001b[0m, in \u001b[0;36mNeuralNet.run_single_epoch\u001b[0;34m(self, iterator, training, prefix, step_fn, **fit_params)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m   1225\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\u001b[39m\"\u001b[39m\u001b[39mon_batch_begin\u001b[39m\u001b[39m\"\u001b[39m, batch\u001b[39m=\u001b[39mbatch, training\u001b[39m=\u001b[39mtraining)\n\u001b[0;32m-> 1226\u001b[0m     step \u001b[39m=\u001b[39m step_fn(batch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1227\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mrecord_batch(prefix \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_loss\u001b[39m\u001b[39m\"\u001b[39m, step[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem())\n\u001b[1;32m   1228\u001b[0m     batch_size \u001b[39m=\u001b[39m (get_len(batch[\u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(batch, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m))\n\u001b[1;32m   1229\u001b[0m                   \u001b[39melse\u001b[39;00m get_len(batch))\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1105\u001b[0m, in \u001b[0;36mNeuralNet.train_step\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\n\u001b[1;32m   1098\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mon_grad_computed\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1099\u001b[0m         named_parameters\u001b[39m=\u001b[39mTeeGenerator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_all_learnable_params()),\n\u001b[1;32m   1100\u001b[0m         batch\u001b[39m=\u001b[39mbatch,\n\u001b[1;32m   1101\u001b[0m         training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[1;32m   1103\u001b[0m     \u001b[39mreturn\u001b[39;00m step[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m-> 1105\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_optimizer(step_fn)\n\u001b[1;32m   1106\u001b[0m \u001b[39mreturn\u001b[39;00m step_accumulator\u001b[39m.\u001b[39mget_step()\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1060\u001b[0m, in \u001b[0;36mNeuralNet._step_optimizer\u001b[0;34m(self, step_fn)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m   1059\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(step_fn)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/optim/adamw.py:148\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 148\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    150\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    151\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1094\u001b[0m, in \u001b[0;36mNeuralNet.train_step.<locals>.step_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_fn\u001b[39m():\n\u001b[1;32m   1093\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_optimizer()\n\u001b[0;32m-> 1094\u001b[0m     step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step_single(batch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m   1095\u001b[0m     step_accumulator\u001b[39m.\u001b[39mstore_step(step)\n\u001b[1;32m   1097\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify(\n\u001b[1;32m   1098\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mon_grad_computed\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1099\u001b[0m         named_parameters\u001b[39m=\u001b[39mTeeGenerator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_all_learnable_params()),\n\u001b[1;32m   1100\u001b[0m         batch\u001b[39m=\u001b[39mbatch,\n\u001b[1;32m   1101\u001b[0m         training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1102\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:994\u001b[0m, in \u001b[0;36mNeuralNet.train_step_single\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n\u001b[1;32m    992\u001b[0m Xi, yi \u001b[39m=\u001b[39m unpack_data(batch)\n\u001b[1;32m    993\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(Xi, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 994\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_loss(y_pred, yi, X\u001b[39m=\u001b[39;49mXi, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    995\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    996\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    997\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: loss,\n\u001b[1;32m    998\u001b[0m     \u001b[39m'\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m'\u001b[39m: y_pred,\n\u001b[1;32m    999\u001b[0m }\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/classifier.py:150\u001b[0m, in \u001b[0;36mNeuralNetClassifier.get_loss\u001b[0;34m(self, y_pred, y_true, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     eps \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfinfo(y_pred\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39meps\n\u001b[1;32m    149\u001b[0m     y_pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(y_pred \u001b[39m+\u001b[39m eps)\n\u001b[0;32m--> 150\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mget_loss(y_pred, y_true, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/skorch/net.py:1665\u001b[0m, in \u001b[0;36mNeuralNet.get_loss\u001b[0;34m(self, y_pred, y_true, X, training)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the loss for this batch.\u001b[39;00m\n\u001b[1;32m   1637\u001b[0m \n\u001b[1;32m   1638\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1662\u001b[0m \n\u001b[1;32m   1663\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m y_true \u001b[39m=\u001b[39m to_tensor(y_true, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1665\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion_(y_pred, y_true)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (100) to match target batch_size (200)."
     ]
    }
   ],
   "source": [
    "clf.fit(step2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1303619598.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    from sklea\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklea\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # return self for compatibility reasons\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Your code here that uses both X and y\n",
    "        print(\"X\", X.shape)\n",
    "        print(\"y\", y.shape)\n",
    "        return X\n",
    "    \n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('custom_transformer', CustomTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/scratch/lgrinszt/lm_tab/scripts/launch.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/lm_tab/scripts/launch.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m X, y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m100\u001b[39m, \u001b[39m10\u001b[39m), np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m100\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/scratch/lgrinszt/lm_tab/scripts/launch.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit(X, y)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    429\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/svm/_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    191\u001b[0m         X,\n\u001b[1;32m    192\u001b[0m         y,\n\u001b[1;32m    193\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[1;32m    194\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    195\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    196\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[1;32m    199\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/utils/validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[0;32m-> 1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1147\u001b[0m     X,\n\u001b[1;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1149\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1150\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1151\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1152\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1153\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1154\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1155\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1156\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1157\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1158\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[1;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/utils/validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    914\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    916\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    918\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    919\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'ellipsis'"
     ]
    }
   ],
   "source": [
    "X, y = np.random.rand(100, 10), np.random.randint(0, 2, 100)\n",
    "\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "batch_size should be a positive integer value, but got batch_size=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb Cell 24\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X16sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m CustomDataset(encoding[train_size\u001b[39m+\u001b[39mval_size:], torch\u001b[39m.\u001b[39mtensor(labels)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)[train_size\u001b[39m+\u001b[39mval_size:])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X16sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_dataset), shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X16sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m val_loader \u001b[39m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(val_dataset), shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X16sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(test_dataset), shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X16sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Initialize model and optimizer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leo/VSCProjects/lm_tab/scripts/launch.ipynb#X16sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m#model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/utils/data/dataloader.py:357\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    353\u001b[0m             sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[39mif\u001b[39;00m batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m batch_sampler \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[39m# auto_collation without custom batch_sampler\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     batch_sampler \u001b[39m=\u001b[39m BatchSampler(sampler, batch_size, drop_last)\n\u001b[1;32m    359\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m    360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_last \u001b[39m=\u001b[39m drop_last\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/torch/utils/data/sampler.py:232\u001b[0m, in \u001b[0;36mBatchSampler.__init__\u001b[0;34m(self, sampler, batch_size, drop_last)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, sampler: Union[Sampler[\u001b[39mint\u001b[39m], Iterable[\u001b[39mint\u001b[39m]], batch_size: \u001b[39mint\u001b[39m, drop_last: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[39m# Since collections.abc.Iterable does not check for `__getitem__`, which\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# is one way for an object to be an iterable, we don't do an `isinstance`\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39m# check here.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_size, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_size, \u001b[39mbool\u001b[39m) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    231\u001b[0m             batch_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 232\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbatch_size should be a positive integer value, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mbut got batch_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(batch_size))\n\u001b[1;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(drop_last, \u001b[39mbool\u001b[39m):\n\u001b[1;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdrop_last should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mdrop_last=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(drop_last))\n",
      "\u001b[0;31mValueError\u001b[0m: batch_size should be a positive integer value, but got batch_size=0"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from src.utils import preprocess_input\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "\n",
    "# Your text data and labels (replace these with your actual data and labels)\n",
    "# Your text data and labels (replace these with your actual data and labels)\n",
    "#texts = X_original[column_to_consider].tolist()\n",
    "texts = X.tolist()\n",
    "#labels = (y_original > np.median(y_original)).tolist()\n",
    "#labels = y_original.tolist()\n",
    "labels = y.tolist()\n",
    "\n",
    "# Tokenize the text data\n",
    "encoding = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Create a custom dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "#dataset = CustomDataset(encoding, torch.tensor(labels).float().reshape(-1, 1))\n",
    "#print(f\"Dataset size: {len(dataset)}\")\n",
    "train_size = 1000\n",
    "val_size = 1000 #TODO\n",
    "#test_size = len(dataset) - train_size - val_size\n",
    "#print(f\"Train size: {train_size}, Val size: {val_size}, Test size: {test_size}\")\n",
    "#train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "#train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "train_dataset = CustomDataset(encoding[:train_size], torch.tensor(labels).float().reshape(-1, 1)[:train_size])\n",
    "val_dataset = CustomDataset(encoding[train_size:train_size+val_size], torch.tensor(labels).float().reshape(-1, 1)[train_size:train_size+val_size])\n",
    "test_dataset = CustomDataset(encoding[train_size+val_size:], torch.tensor(labels).float().reshape(-1, 1)[train_size+val_size:])\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "#model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model = BertAndTabPFN(preprocess_before_tabpfn=True, linear_translator=False).to('cuda')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    ###########\n",
    "    # Train loop\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids_train = batch['input_ids']\n",
    "        attention_mask_train = batch['attention_mask']\n",
    "        labels_train = batch['labels']\n",
    "        # move the inputs to GPU\n",
    "        input_ids_train = input_ids_train.to('cuda')\n",
    "        attention_mask_train = attention_mask_train.to('cuda')\n",
    "        labels_train = labels_train.to('cuda')\n",
    "        single_eval_pos = 500\n",
    "        output = model(input_ids_train, attention_mask=attention_mask_train, y=labels_train, single_eval_pos=single_eval_pos).squeeze()\n",
    "        loss = nn.CrossEntropyLoss()(output, labels_train[single_eval_pos:].long().reshape(-1))\n",
    "        if epoch > 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # compute train accuracy\n",
    "        preds = torch.argmax(output, axis=-1).cpu().detach().numpy()\n",
    "        train_accuracy = accuracy_score(labels_train[single_eval_pos:].cpu().detach().numpy().reshape(-1), preds)\n",
    "        print(f\"Epoch {epoch + 1} - Training loss: {loss}, Training accuracy: {train_accuracy}\")\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_preds, val_labels, val_losses = [], [], []\n",
    "    best_val_loss = np.inf\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader: #TODO: remove the useless for loop\n",
    "            input_ids_val = batch['input_ids']\n",
    "            attention_mask_val = batch['attention_mask']\n",
    "            labels_val = batch['labels']\n",
    "            # move the inputs to GPU\n",
    "            input_ids_val = input_ids_val.to('cuda')\n",
    "            attention_mask_val = attention_mask_val.to('cuda')\n",
    "            labels_val = labels_val.to('cuda')\n",
    "            # concatenate train and val\n",
    "            #TODO: make sure this is correct, no leak etc\n",
    "            # maybe safer to create a TabPFNClassifier with the same parameters as the one in BertAndTabPFN\n",
    "            input_ids = torch.cat((input_ids_train, input_ids_val), axis=0)\n",
    "            attention_mask = torch.cat((attention_mask_train, attention_mask_val), axis=0)\n",
    "            labels = torch.cat((labels_train, labels_val), axis=0)\n",
    "            single_eval_pos = 1000\n",
    "            output = model(input_ids, attention_mask=attention_mask, y=labels, single_eval_pos=single_eval_pos).squeeze()\n",
    "            val_loss = nn.CrossEntropyLoss()(output, labels[single_eval_pos:].long().reshape(-1))\n",
    "            if val_loss < best_val_loss:\n",
    "                print(f\"New best validation loss: {val_loss}\")\n",
    "                best_val_loss = val_loss\n",
    "                # save the model\n",
    "                torch.save(model.state_dict(), \"checkpoints/model.pt\")\n",
    "                # save input_ids_train, attention_mask_train, labels_train\n",
    "                torch.save(input_ids_train, \"checkpoints/input_ids_train.pt\")\n",
    "                torch.save(attention_mask_train, \"checkpoints/attention_mask_train.pt\")\n",
    "                torch.save(labels_train, \"checkpoints/labels_train.pt\")\n",
    "\n",
    "\n",
    "            val_losses.append(val_loss.cpu())\n",
    "            preds = torch.argmax(output, axis=-1).cpu().detach().numpy()\n",
    "            val_preds.append(preds)\n",
    "            val_labels.append(labels[single_eval_pos:].cpu().detach().numpy().reshape(-1))\n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_labels = np.concatenate(val_labels)\n",
    "    val_losses = np.mean(val_losses)\n",
    "    print(f\"Epoch {epoch + 1} - Validation loss: {val_losses}\")\n",
    "    # Compute accuracy\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch + 1} - Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"checkpoints/model.pt\"))\n",
    "input_ids_train = torch.load(\"checkpoints/input_ids_train.pt\")\n",
    "attention_mask_train = torch.load(\"checkpoints/attention_mask_train.pt\")\n",
    "labels_train = torch.load(\"checkpoints/labels_train.pt\")\n",
    "\n",
    "# Test loop\n",
    "model.eval()\n",
    "test_preds, test_labels, test_losses = [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids_test = batch['input_ids']\n",
    "        attention_mask_test = batch['attention_mask']\n",
    "        labels_test = batch['labels']\n",
    "        # move the inputs to GPU\n",
    "        input_ids_test = input_ids_test.to('cuda')\n",
    "        attention_mask_test = attention_mask_test.to('cuda')\n",
    "        labels_test = labels_test.to('cuda')\n",
    "        # concatenate train and val\n",
    "        #TODO put this back\n",
    "        #input_ids = torch.cat((input_ids_train, input_ids_test), axis=0)\n",
    "        #attention_mask = torch.cat((attention_mask_train, attention_mask_test), axis=0)\n",
    "        #labels = torch.cat((labels_train, labels_test), axis=0)\n",
    "        single_eval_pos = 1000\n",
    "\n",
    "        output = model(input_ids_test, attention_mask=attention_mask_test, y=labels_test, single_eval_pos=single_eval_pos).squeeze()\n",
    "        test_loss = nn.CrossEntropyLoss()(output, labels_test[single_eval_pos:].long().reshape(-1))\n",
    "        test_losses.append(test_loss.cpu())\n",
    "        preds = torch.argmax(output, axis=-1).cpu().detach().numpy()\n",
    "        test_preds.append(preds)\n",
    "        test_labels.append(labels_test[single_eval_pos:].cpu().detach().numpy().reshape(-1))\n",
    "\n",
    "test_preds = np.concatenate(test_preds)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "test_losses = np.mean(test_losses)\n",
    "print(f\"Test loss: {test_losses}\")\n",
    "print(f\"Test accuracy: {accuracy_score(test_labels, test_preds)}\")\n",
    "\n",
    "# Save the model\n",
    "# model.save_pretrained(\"./fine_tuned_bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
