{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_high_cardinality_features(X, encoder_name, dataset_name=None, use_cache=True, override_cache=False, cardinality_threshold=30, fail_if_not_cached=False):\n",
    "\n",
    "    # encode the high cardinality columns\n",
    "    res = []\n",
    "    lengths = []\n",
    "    for col in high_cardinality_columns:\n",
    "        new_enc = encode(X, col, encoder_name, dataset_name=dataset_name, use_cache=use_cache, override_cache=override_cache, fail_if_not_cached=fail_if_not_cached)\n",
    "        res.append(new_enc)\n",
    "        lengths.append(new_enc.shape[1])\n",
    "    # create a dataframe with name original_col_name__index\n",
    "    df = pd.DataFrame(np.concatenate(res, axis=1))\n",
    "\n",
    "    #df = pd.DataFrame(np.concatenate(res, axis=1))\n",
    "    # for i in range(len(res)):\n",
    "    #     for j in range(lengths[i]):\n",
    "    #         df.rename(columns={i*lengths[i] + j: high_cardinality_columns[i] + \"__\" + str(j)}, inplace=True)\n",
    "    new_column_names = []\n",
    "    for i in range(len(res)):\n",
    "        for j in range(lengths[i]):\n",
    "            new_column_names.append(high_cardinality_columns[i] + \"__\" + str(j))\n",
    "    df.columns = new_column_names\n",
    "    return df, X.drop(high_cardinality_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loading import load_data\n",
    "from skrub import TableVectorizer, MinHashEncoder\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "def run_with_hv(dataset, analyzer, ngram_range, dim_reduction_name, dim_reduction, cv, model_name, model, cardinality_threshold=30):\n",
    "    X, y = load_data(dataset, max_rows=10000)\n",
    "    tb = TableVectorizer(cardinality_threshold=cardinality_threshold,\n",
    "                        high_card_cat_transformer = \"passthrough\",\n",
    "                        low_card_cat_transformer = \"passthrough\",\n",
    "                        numerical_transformer = \"passthrough\",\n",
    "                        datetime_transformer = \"passthrough\",\n",
    "    ) #just to get the high cardinality columns\n",
    "    tb.fit(X)\n",
    "    # get high cardinality columns\n",
    "    high_cardinality_columns = []\n",
    "    for name, trans, cols in tb.transformers_:\n",
    "        if \"high\" in name:\n",
    "            high_cardinality_columns.extend(cols)\n",
    "            break\n",
    "    print(\"High cardinality columns\", high_cardinality_columns)\n",
    "    all_enc_cols = []\n",
    "    for col in high_cardinality_columns:\n",
    "        vectorizer = HashingVectorizer(analyzer=analyzer, ngram_range=ngram_range)\n",
    "        res = vectorizer.fit_transform(X[col])\n",
    "        all_enc_cols.append(res)\n",
    "\n",
    "    X_rest = X.drop(high_cardinality_columns, axis=1)\n",
    "\n",
    "    rest_trans = TableVectorizer(high_card_cat_transformer = MinHashEncoder(n_components=10, analyzer='char'),\n",
    "                            cardinality_threshold=30)\n",
    "    # cv by hand\n",
    "    # split X\n",
    "    accuracies = []\n",
    "    roc_aucs = []\n",
    "    for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "        print(f\"Fold {i}\")\n",
    "        X_rest_train = rest_trans.fit_transform(X_rest.iloc[train])\n",
    "        X_rest_test = rest_trans.transform(X_rest.iloc[test])\n",
    "        X_high_train, X_high_test = [], []\n",
    "        for j, col in enumerate(high_cardinality_columns):\n",
    "            dim_rec = clone(dim_reduction)\n",
    "            X_high_train.append(dim_rec.fit_transform(all_enc_cols[j][train]))\n",
    "            X_high_test.append(dim_rec.transform(all_enc_cols[j][test]))\n",
    "        X_high_train = np.concatenate(X_high_train, axis=1)\n",
    "        X_high_test = np.concatenate(X_high_test, axis=1)\n",
    "        X_train = np.concatenate([X_rest_train, X_high_train], axis=1)\n",
    "        X_test = np.concatenate([X_rest_test, X_high_test], axis=1)\n",
    "        print(\"X_train shape\", X_train.shape)\n",
    "        print(\"X_test shape\", X_test.shape)\n",
    "        model.fit(X_train, y[train])\n",
    "        y_pred = model.predict(X_test)\n",
    "        # accuracy and roc_auc\n",
    "        accuracy = accuracy_score(y[test], y_pred)\n",
    "        roc_auc = roc_auc_score(y[test], y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        roc_aucs.append(roc_auc)\n",
    "    \n",
    "    return {\n",
    "        \"dataset\": dataset,\n",
    "        \"analyzer\": analyzer,\n",
    "        \"ngram_range\": ngram_range,\n",
    "        \"dim_reduction_name\": dim_reduction_name,\n",
    "        \"dim_reduction\": dim_reduction,\n",
    "        \"cv\": cv,\n",
    "        \"model_name\": model_name,\n",
    "        \"model\": model,\n",
    "        \"accuracy\": accuracies,\n",
    "        \"roc_auc\": roc_aucs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2 columns with missing values on 12 columns\n",
      "Removed 1652 rows with missing values on 3967 rows\n",
      "Removed 1652 rows with missing values on 2315 rows\n",
      "Removed 2 columns with missing values on 9 columns\n",
      "New shape: (2315, 10)\n",
      "Original task: regression for goodreads\n",
      "Converting to binary classification\n",
      "Classes (array([0, 1]), array([1167, 1148]))\n",
      "X shape: (2315, 10), y shape: (2315,)\n",
      "High cardinality columns ['Title', 'Description', 'FirstAuthor', 'NumberofReviews', 'Publisher', 'PublishDate']\n",
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:228: UserWarning: Found unknown categories in columns [0, 1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (1852, 204)\n",
      "X_test shape (463, 204)\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:228: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (1852, 207)\n",
      "X_test shape (463, 207)\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:228: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (1852, 207)\n",
      "X_test shape (463, 207)\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:228: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (1852, 204)\n",
      "X_test shape (463, 204)\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:228: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (1852, 207)\n",
      "X_test shape (463, 207)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': 'goodreads',\n",
       " 'analyzer': 'char',\n",
       " 'ngram_range': (1, 2),\n",
       " 'dim_reduction_name': 'PCA_30',\n",
       " 'dim_reduction': TruncatedSVD(n_components=30),\n",
       " 'cv': StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       " 'model_name': 'GradientBoostingClassifier',\n",
       " 'model': GradientBoostingClassifier(),\n",
       " 'accuracy': [0.6479481641468683,\n",
       "  0.6004319654427646,\n",
       "  0.591792656587473,\n",
       "  0.6220302375809935,\n",
       "  0.6414686825053996],\n",
       " 'roc_auc': [0.6485369312880229,\n",
       "  0.6012018064419811,\n",
       "  0.5921813771225976,\n",
       "  0.6220563537973502,\n",
       "  0.6417615226721404]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "run_with_hv(\"goodreads\", \"char\", (1, 2), \"PCA_30\", TruncatedSVD(n_components=30), StratifiedKFold(n_splits=5), \"GradientBoostingClassifier\", GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2 columns with missing values on 12 columns\n",
      "Removed 1652 rows with missing values on 3967 rows\n",
      "Removed 1652 rows with missing values on 2315 rows\n",
      "Removed 2 columns with missing values on 9 columns\n",
      "New shape: (2315, 10)\n",
      "Original task: regression for goodreads\n",
      "Converting to binary classification\n",
      "Classes (array([0, 1]), array([1167, 1148]))\n",
      "X shape: (2315, 10), y shape: (2315,)\n",
      "numeric ['PageCount', 'NumberofRatings']\n",
      "low_card_cat ['Format', 'Language']\n",
      "high_card_cat ['Title', 'Description', 'FirstAuthor', 'NumberofReviews', 'Publisher', 'PublishDate']\n",
      "High cardinality columns ['Title', 'Description', 'FirstAuthor', 'NumberofReviews', 'Publisher', 'PublishDate']\n",
      "working dir /scratch/lgrinszt/lm_tab/scripts\n",
      "Encoder type hashing_vectorizer\n",
      "Encoder params char_(2,4)\n",
      "Hashing vectorizer shape (2315, 1048576)\n",
      "Saving to cache\n",
      "working dir /scratch/lgrinszt/lm_tab/scripts\n",
      "Encoder type hashing_vectorizer\n",
      "Encoder params char_(2,4)\n",
      "Hashing vectorizer shape (2315, 1048576)\n",
      "Saving to cache\n",
      "working dir /scratch/lgrinszt/lm_tab/scripts\n",
      "Encoder type hashing_vectorizer\n",
      "Encoder params char_(2,4)\n",
      "Hashing vectorizer shape (2315, 1048576)\n",
      "Saving to cache\n",
      "working dir /scratch/lgrinszt/lm_tab/scripts\n",
      "Encoder type hashing_vectorizer\n",
      "Encoder params char_(2,4)\n",
      "Hashing vectorizer shape (2315, 1048576)\n",
      "Saving to cache\n",
      "working dir /scratch/lgrinszt/lm_tab/scripts\n",
      "Encoder type hashing_vectorizer\n",
      "Encoder params char_(2,4)\n",
      "Hashing vectorizer shape (2315, 1048576)\n",
      "Saving to cache\n",
      "working dir /scratch/lgrinszt/lm_tab/scripts\n",
      "Encoder type hashing_vectorizer\n",
      "Encoder params char_(2,4)\n",
      "Hashing vectorizer shape (2315, 1048576)\n",
      "Saving to cache\n",
      "6291456\n",
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 1 elements, new values have 6291456 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoodreads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m X, y \u001b[38;5;241m=\u001b[39m load_data(dataset, max_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m X_enc, X_rest \u001b[38;5;241m=\u001b[39m \u001b[43mencode_high_cardinality_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcardinality_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/lgrinszt/lm_tab/src/encodings.py:246\u001b[0m, in \u001b[0;36mencode_high_cardinality_features\u001b[0;34m(X, encoder_name, dataset_name, use_cache, override_cache, cardinality_threshold, fail_if_not_cached)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(new_column_names))\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m--> 246\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m \u001b[38;5;241m=\u001b[39m new_column_names\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df, X\u001b[38;5;241m.\u001b[39mdrop(high_cardinality_columns, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/pandas/core/generic.py:6002\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   6001\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[0;32m-> 6002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6003\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m   6004\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/pandas/_libs/properties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/pandas/core/generic.py:730\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    729\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/pandas/core/internals/managers.py:225\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[0;32m/scratch/lgrinszt/micromamba/envs/dpo/lib/python3.10/site-packages/pandas/core/internals/base.py:70\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 1 elements, new values have 6291456 elements"
     ]
    }
   ],
   "source": [
    "from src.data_loading import load_data\n",
    "from src.encodings import encode, encode_high_cardinality_features\n",
    "encoding = 'hashing_vectorizer__char_(2,4)'\n",
    "dataset = \"goodreads\"\n",
    "X, y = load_data(dataset, max_rows=10000)\n",
    "X_enc, X_rest = encode_high_cardinality_features(X, encoding, dataset_name=dataset, override_cache=True, cardinality_threshold=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"EleutherAI/pythia-70m\",\n",
    "    \"EleutherAI/pythia-160m\",\n",
    "    \"EleutherAI/pythia-410m\",\n",
    "    \"EleutherAI/pythia-1b\",\n",
    "    \"EleutherAI/pythia-1.4b\",\n",
    "    \"EleutherAI/pythia-2.8b\",\n",
    "    \"EleutherAI/pythia-6.9b\",\n",
    "]\n",
    "for model_name in model_names:\n",
    "    encodings.append(f\"lm__{model_name}\")\n",
    "\n",
    "\n",
    "datasets = ['bikewale', 'clear_corpus', 'company_employees',\n",
    "       'employee-remuneration-and-expenses-earning-over-75000',\n",
    "       'employee_salary', 'goodreads', 'journal_jcr_cls', 'ramen_ratings',\n",
    "       'spotify', 'us_accidents_counts', 'us_accidents_severity',\n",
    "       'us_presidential', 'wine_review', 'zomato']\n",
    "\n",
    "\n",
    "\n",
    "executor = submitit.AutoExecutor(folder=\"logs\")\n",
    "executor.update_parameters(timeout_min=300, slurm_partition='parietal,normal,gpu',\n",
    "                           exclude=\"margpu001,margpu002,margpu003,margpu004\",\n",
    "                           gpus_per_node=1)\n",
    "\n",
    "def encoding_dataset(dataset, encoding):\n",
    "    X, y = load_data(dataset, max_rows=10000)\n",
    "    #X_enc = encode(X_text, encoding, dataset_name=dataset, override_cache=False)\n",
    "    X_enc, X_rest = encode_high_cardinality_features(X, encoding, dataset_name=dataset, override_cache=False, cardinality_threshold=30)\n",
    "    #return X_enc, X_rest, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 columns with missing values on 8 columns\n",
      "Removed 2 rows with missing values on 9003 rows\n",
      "Removed 2 rows with missing values on 9001 rows\n",
      "Removed 0 columns with missing values on 7 columns\n",
      "New shape: (9001, 8)\n",
      "Original task: regression for bikewale\n",
      "Converting to binary classification\n",
      "Classes (array([0, 1]), array([4645, 4356]))\n",
      "X shape: (9001, 8), y shape: (9001,)\n",
      "numeric ['km_driven', 'model_year']\n",
      "low_card_cat ['city_posted', 'fuel_type', 'owner_type']\n",
      "high_card_cat ['bike_name', 'color', 'url']\n",
      "High cardinality columns ['bike_name', 'color', 'url']\n",
      "working dir /scratch/lgrinszt/lm_tab/scripts\n",
      "Encoder type hf\n",
      "Encoder params EleutherAI/pythia-70m\n",
      "Encoding with HF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting padding token\n",
      "Saving to cache\n",
      "working dir /scratch/lgrinszt/lm_tab/scripts\n",
      "Encoder type hf\n",
      "Encoder params EleutherAI/pythia-70m\n",
      "Encoding with HF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting padding token\n",
      "Saving to cache\n",
      "working dir /scratch/lgrinszt/lm_tab/scripts\n",
      "Encoder type hf\n",
      "Encoder params EleutherAI/pythia-70m\n",
      "Encoding with HF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting padding token\n",
      "Saving to cache\n"
     ]
    }
   ],
   "source": [
    "from src.data_loading import load_data\n",
    "from src.encodings import encode, encode_high_cardinality_features\n",
    "encoding = \"hf__EleutherAI/pythia-70m\"\n",
    "dataset = \"bikewale\"\n",
    "X, y = load_data(dataset, max_rows=10000)\n",
    "X_enc, X_rest = encode_high_cardinality_features(X, encoding, dataset_name=dataset, override_cache=True, cardinality_threshold=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "\n",
    "#Sentences we want sentence embeddings for\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "             'Sentences are passed as a list of string.',\n",
    "             'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "model = AutoModel.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "\n",
    "# Set padding token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# move to gpu if available\n",
    "if torch.cuda.is_available():\n",
    "    encoded_input.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "from torch.utils.data import DataLoader\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "model = AutoModel.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "\n",
    "# Set padding token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Make sure model and tokenizer are on the same device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# Create a DataLoader to handle batching of the sentences\n",
    "sentences_loader = DataLoader(sentences, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# List to store all embeddings\n",
    "all_embeddings = []\n",
    "\n",
    "for sentence_batch in sentences_loader:\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentence_batch, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "    # Move tensors to the same device as the model\n",
    "    encoded_input = encoded_input.to(device)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling. In this case, mean pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Move embeddings to CPU, convert to numpy and store\n",
    "    all_embeddings.extend(sentence_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sentence similarity\n",
    "sentences = ['The cat sits outside',\n",
    "             \"The cat sits inside\",\n",
    "]\n",
    "\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "for sent in sentences:\n",
    "    print(\"\\n\" + sent)\n",
    "    for i, sent_emb in enumerate(sentence_embeddings):\n",
    "        print(f\"{i}: {sent_emb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autofj.datasets import load_data\n",
    "#from skrub import fuzzy_join\n",
    "from src.fuzzy_join_custom import fuzzy_join\n",
    "from src.encodings import encode, get_batch_embeddings\n",
    "from autofj import AutoFJ\n",
    "def autofj_merge(left, right, target=0.9):\n",
    "    \"\"\"Merging using AutomaticFuzzyJoin\"\"\"\n",
    "    autofj = AutoFJ(precision_target=target, verbose=True)\n",
    "    autofj_joins = autofj.join(left, right, id_column=\"id\")\n",
    "    return autofj_joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all directory names at https://github.com/chu-data-lab/AutomaticFuzzyJoin/tree/master/src/autofj/benchmark\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "url = \"https://api.github.com/repos/chu-data-lab/AutomaticFuzzyJoin/contents/src/autofj/benchmark\"\n",
    "response = urlopen(url)\n",
    "data = json.loads(response.read())\n",
    "print(\"Available datasets:\")\n",
    "dataset_list = []\n",
    "for d in data:\n",
    "    dataset_list.append(d[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import submitit\n",
    "executor = submitit.AutoExecutor(folder=\"logs\")\n",
    "executor.update_parameters(timeout_min=100, slurm_partition='parietal,normal', slurm_array_parallelism=100, cpus_per_task=2,\n",
    "                            exclude=\"margpu009,marg00[1-9],marg0[11-12],marg0[14-15],marg0[16-20],marg0[25-32]\")\n",
    "# change name of job\n",
    "executor.update_parameters(name=\"pipeline\")\n",
    "\n",
    "encodings = [\"openai__\", \"lm__BAAI/bge-large-en-v1.5\", \"lm__llmrails/ember-v1\"]\n",
    "\n",
    "def encode_join(dataset, column, encoder_name):\n",
    "    \"\"\"Encode a dataset and save it to disk\"\"\"\n",
    "    left_table, right_table, gt = load_data(dataset)\n",
    "    # encode both tables with fasttext (column title)\n",
    "    main_str_enc = encode(left_table, column, encoder_name=encoder_name, dataset_name=\"join_left_\" + dataset)\n",
    "    aux_str_enc = encode(right_table, column, encoder_name=encoder_name, dataset_name=\"join_right_\" + dataset)\n",
    "    return main_str_enc, aux_str_enc\n",
    "\n",
    "with executor.batch():\n",
    "    for encoding in encodings:\n",
    "        for dataset in dataset_list:\n",
    "            for column in [\"title\"]:\n",
    "                executor.submit(encode_join, dataset, column, encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset=\"PoliticalParty\"\n",
    "encoder_name=\"lm__BAAI/bge-large-en-v1.5\"\n",
    "match_score = 0.3\n",
    "left_table, right_table, gt = load_data(dataset)\n",
    "# encode both tables with fasttext (column title)\n",
    "main_str_enc = encode(left_table, column, encoder_name=encoder_name, dataset_name=\"join_left_\" + dataset)\n",
    "aux_str_enc = encode(right_table, column, encoder_name=encoder_name, dataset_name=\"join_right_\" + dataset)\n",
    "\n",
    "joined_fj = fuzzy_join(left_table, right_table, left_on=\"title\",\n",
    "            match_score=match_score,\n",
    "            right_on=\"title\",\n",
    "            return_score=True,\n",
    "            suffixes=(\"_l\", \"_r\"),\n",
    "            main_str_enc_arg=main_str_enc,\n",
    "            aux_str_enc_arg=aux_str_enc,\n",
    ")\n",
    "\n",
    "df_all = pd.merge(joined_fj, gt, on=[\"id_l\"], suffixes=(\"\", \"_gt\"))\n",
    "df_all[\"correct\"] = df_all[\"id_r\"] == df_all[\"id_r_gt\"]\n",
    "# drop na in correct\n",
    "df_all = df_all.dropna(subset=[\"correct\"])\n",
    "\n",
    "recall = df_all[\"correct\"].sum() / len(gt)\n",
    "precision = df_all[\"correct\"].sum() / len(df_all)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"text\": [\"Mary, Queen of the World, Cathedral\", \"Mary, Queen of the World Cathedral\", \"Birmingham Orthodox Cathedral\"]})\n",
    "encode(df, \"text\", \"openai__encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embed_1, embed_2, embed_3 = get_batch_embeddings([\"Mary, Queen of the World, Cathedral\", \"Mary, Queen of the World Cathedral\", \"Birmingham Orthodox Cathedral\"])#, model=\"text-embedding-ada-002\")\n",
    "# compute distances\n",
    "dist_1_2 = np.linalg.norm(embed_1 - embed_2)\n",
    "dist_1_3 = np.linalg.norm(embed_1 - embed_3)\n",
    "dist_2_3 = np.linalg.norm(embed_2 - embed_3)\n",
    "print(\"Distances between pairs of strings:\")\n",
    "print(f\"1-2: {dist_1_2}\")\n",
    "print(f\"1-3: {dist_1_3}\")\n",
    "print(f\"2-3: {dist_2_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"HistoricBuilding\"\n",
    "left_table, right_table, gt = load_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode both tables with fasttext (column title)\n",
    "main_str_enc = encode(left_table, \"title\", encoder_name=\"openai__\", dataset_name=\"join_left_\" + dataset)\n",
    "aux_str_enc = encode(right_table, \"title\", encoder_name=\"openai__\", dataset_name=\"join_right_\" + dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_fj = fuzzy_join(left_table, right_table, left_on=\"title\",\n",
    "            match_score=0.7,\n",
    "            right_on=\"title\",\n",
    "            return_score=True,\n",
    "            suffixes=(\"_l\", \"_r\"),\n",
    "            main_str_enc_arg=main_str_enc,\n",
    "            aux_str_enc_arg=aux_str_enc,\n",
    ")\n",
    "\n",
    "joined_fj_autofj = autofj_merge(\n",
    "            left_table,\n",
    "            right_table,\n",
    "            target=0.5,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_fj_autofj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(joined_fj_autofj, gt, on=[\"id_l\"], suffixes=(\"\", \"_gt\"))\n",
    "df_all[\"correct\"] = df_all[\"id_r\"] == df_all[\"id_r_gt\"]\n",
    "# replace nans by False in correct column\n",
    "#df_all[\"correct\"] = df_all[\"correct\"].fillna(False)\n",
    "# drop nans in correct column\n",
    "#df_all = df_all.dropna(subset=[\"correct\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"correct\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"correct\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_fj[joined_fj[\"title_r\"] == \"Westminster Central Hall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_fj_autofj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import evaluate\n",
    "\n",
    "pr, re, f1 = evaluate(\n",
    "            list(zip(joined_fj[\"title_l\"], joined_fj[\"title_r\"])),\n",
    "            list(zip(gt[\"title_l\"], gt[\"title_r\"])),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision: {pr:.3f}, Recall: {re:.3f}, F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision: {pr:.3f}, Recall: {re:.3f}, F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from skrub import MinHashEncoder, TableVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import submitit\n",
    "from src.data_loading import load_data\n",
    "from src.utils import FixedSizeSplit\n",
    "from itertools import product\n",
    "import time\n",
    "import os\n",
    "\n",
    "def run_catboost(X, y, cv, features, **kwargs):\n",
    "    \"\"\"\n",
    "    X_text: np array of shape (n_samples, 1), the text feature\n",
    "    X_rest: np array of shape (n_samples, n_features), additional tabular data\n",
    "    y: np array of shape (n_samples,), the classifcation target\n",
    "    dim_reduction_name: str, the name of the dim reduction method\n",
    "    dim_reduction: sklearn transformer, the dim reduction method\n",
    "    model_name: str, the name of the model\n",
    "    model: sklearn model, the model\n",
    "    encoding: str, the name of the encoding which was used to create X_enc\n",
    "    cv: sklearn cross validator, the cross validator to use\n",
    "    \"\"\"\n",
    "    tb = TableVectorizer(cardinality_threshold=len(X) * 0.8, #TODO: don't hardcode\n",
    "                        high_card_cat_transformer = \"passthrough\",\n",
    "                        low_card_cat_transformer = \"passthrough\",\n",
    "                        numerical_transformer = \"passthrough\",\n",
    "                        datetime_transformer = \"passthrough\",\n",
    "    ) #just to get the high cardinality columns\n",
    "    tb.fit(X)\n",
    "    # get high cardinality columns\n",
    "    high_cardinality_columns = []\n",
    "    low_cardinality_columns = []\n",
    "    datetime_columns = []\n",
    "    for name, trans, cols in tb.transformers_:\n",
    "        print(name, cols)\n",
    "        if \"high\" in name:\n",
    "            high_cardinality_columns.extend(cols)\n",
    "        elif \"low\" in name:\n",
    "            low_cardinality_columns.extend(cols)\n",
    "        elif \"datetime\" in name:\n",
    "            datetime_columns.extend(cols)\n",
    "    print(\"Low cardinality columns\", low_cardinality_columns)\n",
    "    print(\"High cardinality columns\", high_cardinality_columns)\n",
    "    print(\"Datetime columns\", datetime_columns)\n",
    "\n",
    "    # drop datetime columns\n",
    "    X = X.drop(columns=datetime_columns)\n",
    "    \n",
    "\n",
    "    model = CatBoostClassifier()\n",
    "\n",
    "    if features == \"text_only\":\n",
    "        X = X.drop(columns=[col for col in X.columns if (col not in high_cardinality_columns)])\n",
    "    else:\n",
    "        assert features == \"all\"\n",
    "    \n",
    "    # do cv by hand\n",
    "    accuracies = []\n",
    "    roc_aucs = []\n",
    "    #TODO, maybe I can specify in the model and use sklearn cross_val score\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train, cat_features=low_cardinality_columns, text_features=high_cardinality_columns)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        roc_aucs.append(roc_auc)\n",
    "\n",
    "    return {\n",
    "        'encoding': \"catboost\",\n",
    "        'dim_reduction': \"none\",\n",
    "        'model': \"catboost\",\n",
    "        'accuracies': accuracies,\n",
    "        'roc_auc': roc_aucs,\n",
    "        'n_train': cv.n_train,\n",
    "        'n_test': cv.n_test,\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "def pipeline(config):#dataset, encoding, n_test, dim_reduction_name, model_name, n_train, features):\n",
    "    print(config)\n",
    "    dataset, n_test, n_train, features = config\n",
    "\n",
    "    X, y = load_data(dataset, max_rows=10000)\n",
    "    X = X.reset_index()\n",
    "    cv = FixedSizeSplit(n_splits=7, n_train=n_train, n_test=n_test, random_state=42)\n",
    "    return run_catboost(X, y, cv, dataset=dataset, features=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost.datasets import rotten_tomatoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rotten_tomatoes()[0]\n",
    "# drop missing\n",
    "df = df.dropna(axis=0).reset_index()\n",
    "y = df[\"rating\"]\n",
    "X = df.drop(columns=[\"rating\", \"rating_10\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"zomato\"\n",
    "n_train = 2000\n",
    "n_test = 500\n",
    "features = \"all\"\n",
    "X, y = load_data(dataset, max_rows=10000)\n",
    "X = X.reset_index()\n",
    "cv = FixedSizeSplit(n_splits=7, n_train=n_train, n_test=n_test, random_state=42)\n",
    "run_catboost(X, y, cv, dataset=dataset, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.encodings import encode_high_cardinality_features\n",
    "from src.data_loading import load_data\n",
    "\n",
    "dataset = \"goodreads\"\n",
    "X, y = load_data(dataset, max_rows=10_000)\n",
    "encoding = \"openai__\"\n",
    "X_enc, X_rest = encode_high_cardinality_features(X, encoding, dataset_name=dataset, override_cache=False, cardinality_threshold=30, fail_if_not_cached=True)\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, GradientBoostingClassifier, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from src.utils import FixedSizeSplit\n",
    "cv = FixedSizeSplit(n_train=1000, n_test=500, n_splits=3)\n",
    "#run_on_encoded_data_with_cst(X_enc, X_rest, y, \"passthrough\", \"passthrough\", \"HistGradientBoostingClassifier\", HistGradientBoostingClassifier(), encoding, cv=cv, regression=False, no_interaction_between_enc_and_rest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import run_on_encoded_data_ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "run_on_encoded_data_ensemble(X_enc, X_rest, y, \"passthrough\", \"passthrough\", enc_model_name=\"LogisticRegression\", enc_model=LogisticRegression(),\n",
    "                             rest_model_name=\"GradientBoostingClassifier\", rest_model=GradientBoostingClassifier(), encoding=encoding, cv=cv,\n",
    "                             aggregation=\"stacking\",\n",
    "                                regression=False, no_interaction_between_enc_and_rest=False, n_jobs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from src.utils import run_on_encoded_data\n",
    "run_on_encoded_data(X_enc, X_rest, y, \"PCA_30\", PCA(n_components=30),\n",
    "                             model_name=\"GradientBoostingClassifier\", model=GradientBoostingClassifier(), encoding=encoding, cv=cv,\n",
    "                                regression=False, no_interaction_between_enc_and_rest=False, n_jobs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loading import load_data\n",
    "from skrub import TableVectorizer\n",
    "\n",
    "# populate the table with features on each column\n",
    "datasets = ['bikewale', 'clear_corpus', 'company_employees',\n",
    "       'employee-remuneration-and-expenses-earning-over-75000',\n",
    "       'employee_salary', 'goodreads', 'journal_jcr_cls', 'ramen_ratings',\n",
    "       'spotify', 'us_accidents_counts', 'us_accidents_severity',\n",
    "       'us_presidential', 'wine_review', 'zomato']\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    print(\"--------------\")\n",
    "    X, y = load_data(dataset, max_rows=10_000)\n",
    "    tb = TableVectorizer(cardinality_threshold=30,\n",
    "                        high_card_cat_transformer = \"passthrough\",\n",
    "                        low_card_cat_transformer = \"passthrough\",\n",
    "                        numerical_transformer = \"passthrough\",\n",
    "                        datetime_transformer = \"passthrough\",\n",
    "    ) #just to get the high cardinality columns\n",
    "    tb.fit(X)\n",
    "    # get high cardinality columns\n",
    "    high_cardinality_columns = []\n",
    "    for name, trans, cols in tb.transformers_:\n",
    "        print(name, cols)\n",
    "        if \"high\" in name:\n",
    "            high_cardinality_columns.extend(cols)\n",
    "            break\n",
    "    print(\"High cardinality columns\", high_cardinality_columns)\n",
    "    # find all columns for this dataset\n",
    "    # for each column, get the features\n",
    "    for col in high_cardinality_columns:\n",
    "        # get the features for this column\n",
    "        print(col)\n",
    "        print(X[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
